# Advent of Code 2025

- https://adventofcode.com/
- https://github.com/livexia/advent-of-code-2025

## Day 1

今天的问题算是轻松，输入的每一行决定旋钮的转向和旋转的距离，输入的处理也不复杂，但是要注意在解析数字时的错误处理，输入处理完成后，如果旋钮是向左旋转则距离为负数，向右则为正数。

第一部分只需要计算旋转之后的刻度位置是否为 0 即可，可以简单的求 100 的余数即可。

第二部分稍微复杂一些，需要计算每次旋转过程中经过了多少次 0 刻度位置，给出的测试用例较小，实际的输入旋转的距离可能是多圈的，要考虑到这一点。首先计算每一次旋转的距离至少有多少圈，每转一圈密码加一。接着计算整圈之外的旋转距离，如果起始点不在 0 ，那么旋转结束后，如果刻度跨过 0 刻度，密码加一。因为向左旋转距离是负数，如果当前位置不为 0 ，同时当前位置加上旋转距离为负数，则旋钮指针一定向左跨过一次 0 。同理如果当前位置不为 0 （不为 100），同时向右旋转，而旋转后的刻度数超过 100 ，那么指针一定向右旋转过一次 0。根据这个逻辑对 password 进行加一即可。

第二部分代码

```rust
fn part2(rotations: &[i32]) -> Result<u32> {
    let _start = Instant::now();

    let mut dial = 50;
    let mut password = 0;

    for rot in rotations {
        // 计算一定会经过 0 刻度的整圈次数
        password += rot.unsigned_abs() / 100;

        // 扣除整圈的旋转距离，
        // 可以规避例如从 0 刻度旋转距离 100 的边界情况
        let rot = rot % 100;

        let temp = dial + rot;
        // 当起点不是 0 刻度时，
        // 向左或向右旋转超过或位于刻度边界 0 或 100 时，
        // 一定经过一次 0 刻度
        password += (dial != 0 && (temp >= 100 || temp <= 0)) as u32;

        // 计算旋转后刻度的真实位置，刻度值一定大于 0
        dial = temp.rem_euclid(100);
    }

    println!("part2: {password}");
    println!("> Time elapsed is: {:?}", _start.elapsed());
    Ok(password)
}
```

## Day 2

今天要求找出区间内存在重复模式的数字，第一部分要求找出左右两个部分相同的数字，第二部分要求找出数字经过 N 等分后，每个部分都相同的数字。第二部分其实是第一部分的衍生，解题思路在两个部分是一致的。我首先利用暴力法得出题解，通过取余的方法不断的分割数字，对比分割的结果，即可确定是否存在重复模式。暴力法效率不高，因为我遍历了区间内的所有数字，依次检查数字，这样实际效率很慢。

### 渐进寻找

暴力法中区间内的数字是依次递增的，但是应该有更加高效的方法确定符合重复模式的数字。考虑区间 565653-565659 ，可见这个区间中所有的数字都是 6 位等长的，我们先考虑第一部分的重复模式，即数字的前半部分和后半部分应当相同。考虑区间起点 565653 ，直接将区间按照重复模式进行分割，可得到两个数字 565 和 653 ，同样的将结尾也进行分割得到 565 和 659 ，可见两个数字的前半部分相同，那么这个部分就不能变动，那么 653 和 659 就应该变化为 565 ，得到的数字是 565565 不在区间内。所以按照第一部分的匹配模式，无法从区间内寻找到符合的数字。那么考虑第二部分的匹配模式，这个区间的数字长度都为 6 ，那么存在长度为 1、2 和 3 的三种分割模式。确定区间起点和结尾数字中，共同的部分为 56565，那么根据这个共同部分进行分割，可以发现长度 1 或 3 的分割模式是不可能的。考虑分割长度为 2 ，那么分割后每个部分都需要是 56 才行，同时 56 刚好落入起点和结尾 53 - 59 之间，那么长度为 2 的分割可行。

这个方法存在一种情况，那就是如果区间的数字长度不一致，比如区间 95-115 就不容易寻找了，当然可以把区间进行拆分，95-115 变成 95-99 和 100-115 两个区间，分开寻找即可。

**效率对比**

```
part1: 26255179562
> Time elapsed is: 22.953333ms
part2: 31680313976
> Time elapsed is: 37.9495ms
part1 by step: 26255179562
> Time elapsed is: 58.208µs
part2 by step: 31680313976
> Time elapsed is: 70.834µs
```

渐进查找主要代码
```rust
fn split_range(start: usize, end: usize) -> Vec<(usize, usize)> {
    let (start_l, end_l) = (start.ilog10(), end.ilog10());
    if start_l < end_l {
        let mut ranges = vec![];
        let mut start = start;
        for i in start_l..=end_l {
            let new_end = 10usize.pow(i + 1) - 1;
            ranges.push((start, new_end.min(end)));
            start = new_end + 1;
        }
        ranges
    } else {
        vec![(start, end)]
    }
}

fn find_invalid(start: usize, end: usize, base: u32) -> Vec<usize> {
    assert_eq!(start.ilog10(), end.ilog10());
    let l = start.ilog10() + 1;
    if !l.is_multiple_of(base) {
        return vec![];
    }
    let (start_left, end_left) = (start / 10usize.pow(l - base), end / 10usize.pow(l - base));
    let mut invalids = Vec::new();
    for s in start_left..=end_left {
        let n = (0..l)
            .step_by(base as usize)
            .fold(0, |n, i| n + s * 10usize.pow(i));
        if start <= n && n <= end {
            invalids.push(n);
        }
    }
    invalids
}
```

## Day 3

今天要求计算一个数字序列的最大子串（数字），第一部分限定子串长度为2，第二部分则限定长度为12，同时子串的顺序不变。输入的处理不复杂，思路也很简单，就是遍历数字序列，依次寻找最大值即可。

考虑数字序列 96781 ，需要寻找长度为 2 的最大子串，假设子串为 ab ，那么优先从给定序列中确定最大的 a ，同时确保能找到 b 即可。搜索 a 时从左到右进行搜索，而搜索到最后一个数字之前就需要停止，也就是搜索 9678 即可。在搜索 a 的过程中不必考虑是否可能会导致 b 的值不是最大，因为最后的要求的是 ab 最大即可。假设搜索过程中为了确保 b 的值为最大的 d ，而导致 a 取得了较小的值 c，即 c < a 且 d > b ，那么 10 * c + d 一定小于 10 * a + b 。

两个部分对于子串的长度要求不同，但是思路是一致的。从左到右依次搜索字串最大值的过程中，需要记录第一次遇到最大值的元素位置，而非其他可能遇到最大值的位置，这样可以避免影响后续元素最大值搜索。参考序列 98975，需要寻找长度为 2 的最大子串，a 确定搜索到的最大值为 9，如果 a 记录的最大值位置不为 0 而为 2 ，那么搜索 b 时就会从位置 3 开始搜索，最后得到子串为 97 ，是错误结果。

贪心算法核心代码

```rust
fn find_largest_joltage(battery: &[usize], number: usize) -> usize {
    let length = battery.len();
    let mut joltage = 0;
    let mut next_battery = 0;
    for l in (0..number).rev() {
        let mut max_battery = 0;
        (next_battery..(length - l)).for_each(|left| {
            if battery[left] > max_battery {
                max_battery = battery[left];
                next_battery = left + 1;
            }
        });
        joltage = joltage * 10 + max_battery;
    }
    joltage
}
```

这个算法还可以进行剪枝，因为输入的数字序列限制，实际上如果搜索到最大数字为 9 就不需要继续搜索了，这可以进行一定的剪枝，但是我的实现用了 for_each 就具体实现了。

看到一个基本思路一致，但是实现上不同的方法 [ropewalker](https://www.reddit.com/r/adventofcode/comments/1pcvaj4/comment/ns0smcw) [code](https://github.com/ropewalker/advent_of_code_2025/blob/master/src/day03.rs) ，通过滑动限定长度窗口，依次取得最大值，实现不同但算法是一样的，效率应该也没有差别。

目前我的实现在查找过程中实际上存在大量的浪费，因为必须要比较到最后才能确定最大值，而每次查找最大值时都进行了一次。社区上也有人使用 DP 进行优化这个过程，即在一次比较中不断记录当前位置到末尾处的最大值，具体我就不实现了，做一下分析。

阅读社区上他人的动态规划代码 [michel-kraemer](https://github.com/michel-kraemer/adventofcode-rust/blob/main/2025/day03/src/main.rs)，这个实现使用的是自底向上的动态规划，过程中数组 dp 记录的是，长度为 len 的子串从 i 开始的最大子串，转移方程是 dp[len][i] = max(number[i]*mul + dp[len - 1][i + 1])，dp 从长度 1 以及从数字序列最后开始计算。

参考核心代码

```rust
fn dp_find_largest_joltage(battery: &[usize], number: usize) -> usize {
    let length = battery.len();
    let mut dp = vec![vec![0; length + 1]; number + 1];
    let mut mul = 1;
    for len in 1..=number {
        let mut max = 0;
        for (i, &b) in battery.iter().enumerate().take(length - len + 1).rev() {
            max = max.max(b * mul + dp[len - 1][i + 1]);
            dp[len][i] = max;
        }
        mul *= 10;
    }
    dp[number][0]
}
```

理论上 dp 应该要能减少比较的次数，但是这个实现实际的运行效率远不如我的暴力法，对比核心代码实际可以发现比较的次数并没有减少，虽然运行时都是 O(Nk)，N是序列长度，k是子串长度，但是增加了数组操作，整体反而增加了计算成本。比较次数没有减少，是因为每次计算 dp 时，依旧需要从初始位置开始比对到当前的结尾位置，冗余的比较依旧存在。

动态规划算法的核心

- dp[len][i]=从 battery[i..] 中选择 len 个数字，保持相对顺序串联得到的最大值。
- 边界条件（已初始化）：dp[0][i]=0：从任何位置开始选择 0 个数字，结果都是 0。
- 外部循环：按长度 (len) 迭代 `for len in 1..=number` 这个循环是按构建数字的位数从小到大进行计算。DP 算法的关键是利用小规模子问题的解来推导大规模问题的解。在这里，要计算 dp[len][i]，我们必须先知道 dp[len−1][j] 的值。
- 内部循环：按起始索引 (i) 迭代 `for (i, &b) in battery.iter().enumerate().take(length - len + 1).rev()`
- 优化原理：当我们在计算 dp[len][i] 时，我们在考虑从 battery[i..] 中选择 len 个数字。有两种情况：
    - 选择 battery[i] 作为第一个数字： 得到的数字是 battery[i]⋅mul+dp[len−1][i+1]。
    - 不选择 battery[i] 作为第一个数字： 这意味着第一个数字是从 battery[i+1..] 中选的，所以这等价于 dp[len][i+1]。

当前解决的这个特定问题，贪心算法在运行效率（常数时间和空间）上是更优的选择。但在算法设计的普适性上，DP 算法则更为强大。

## Day 04

今天的问题简洁明了，输入是二维的矩阵，其中每个位置上可能存在一卷纸，如果一个卷纸的八个邻接位置上只有小于三个位置上是卷纸时，叉车可以将卷纸移除。第一部分是移除一次，第二部分则是反复进行操作，直到矩阵中所有的卷纸都无法再被移除。问题的核心就是计算矩阵中每个卷纸邻接卷纸的个数，然后根据需求修改矩阵状态。

最直接的思路就是利用二位数组，直接将输入的字符串，转为二维字符数组，接着遍历每一个位置，计算八个邻接位置，判断其中是卷纸的数量。第二部分则需要多次遍历，每次遍历中需要记录移除的卷纸坐标，当一次遍历完成后，根据记录的坐标修改矩阵，将卷纸移除。思路简单明了，也得到了正确的答案，但是我对运行效率不满意，第一部分运行时间是 22ms ，而第二部分则是 273 ms。

第一部分的时间复杂度是 O(N) 其中 N 是矩阵大小，而第二部分的时间复杂度是 O(kN) 其中 k 时可能需要的遍历移除次数，k 最大为最初的卷纸数量。

二维数组模拟输入，实际上非常直观，但是数组存在使用上的局限性，比如八个邻接位置的判断，需要考虑边界条件。同时在这个问题中，第二部分需要搜索的矩阵大小实际上应该是逐渐减小的，而用数组表示则无法利用这个优势。用 HashSet 表示在实现上更加方便，集合只记录所有卷纸的坐标，搜索邻接八个位置时，只需要判断位置是否位于集合中，不必额外考虑坐标边界情况。虽然在实现上更加简便，但是效率却变差了，虽然时间复杂度没有变化，但是 HashSet 的操作更加耗时，最后两部分的运行时间分别是 45ms 和 1s。即便是优化了部分 HashSet 方法的使用，最后的运行时间依旧比不上数组，两部分分别只能做到 48ms 和 437 ms。从第一部分的运行时间差距可以发现，实际上 HashSet 的效率是远比不上数组访问。

### 初步优化（第二部分）

第一部分实际上没有什么优化空间，在这个问题中 Vec 的效率远比 Hash 效率高，优化的主要集中在第二部分。第二部分中移除是分批实现的，即先判断所有的卷纸是否能被移除，然后一次性移除，接着再次判断和移除。这样的实现可以确保结果正确性，但是能否判断一个就移除一个呢？因为所有的卷纸只可能会被移除，不会有新的卷纸产生，如果一个卷纸目前有四个邻接的卷纸，而其中一个卷纸被移除了，那么不需要等到下一次判断，这次就可以一并移除，虽然也会有漏网之鱼，但也能较大的提高运行效率。

实现这个优化采用二维矩阵比 HashSet 更加方便，因为在 Rust 中一边遍历 HashSet 一边又要删除元素是无法实现的（safe），一般来说也不推荐这样做，因为修改遍历对象属于遍历副作用，可能会导致循环无法结束。

在利用 Vec 实现这个优化之后，运行时间并没有大幅下降，第二部分运行时间是 173 ms，差不多少了 100ms。

### 进一步优化

运行效率不高的原因是，在每一个卷纸位置上进行搜索时，需要对领接的八个位置都进行判断，而这个过程又需要反复进行。如果能将搜索邻接位置的结果进行缓存，或者不需要每次都进行判断应该就能大幅降低运行时间。

考虑使用 HashMap 表示输入，其键为卷纸坐标，值为邻接的八个位置中有几个是卷纸。需要提前从输入构筑，初始的邻接搜索必须要进行。每一次搜索时，遇到一个卷纸位置，如果当前卷纸可以被移除，那么邻接八个位置在 HashMap 中的值就需要减 1 。

实际上也可以使用 Vec 二维数组实现，考虑 HashMap 可能存在操作的开销。但是这个思路可以进一步延伸，在搜索一个位置时，如果能提前判断邻接的八个位置是否能被移除，那就可以提前的确定这个卷纸是否能被移除。当然要规避如果 A 移除了，邻接的 B 就能被移除，同时如果 B 被移除，A 也就能被移除，这样的互相限制。

这样的优化终归是局部性的，并不是一劳永逸的，同时是有些盲目的操作。进一步我又想到，实现这个优化最好的数据结构是优先队列，键依旧是坐标，值依旧是邻接的卷纸数量。这个思路看似不错，但是并不容易，因为我需要快速的查找最小值，同时又要求 O(1) 的根据键并修改优先级。可以使用 HashMap + BinaryHeap 实现，HashMap 实时记录位置处邻接卷纸的数量，BinaryHeap 是最小堆，如果堆顶的值大于 4 则说明所有能被移除的卷纸已经被全部移除了。代码具体实现可见 [code](https://github.com/livexia/advent-of-code-2025/commit/3ada45bc2ee0c1cb4a47232aae5d158a766f7660#diff-9babad5b20f1c504f9c4b1d031fd57e6f1a1198343b707f377d777b64c50f47b)，第二部分的运行时间下降到 70ms。

### 使用 BFS 实现

前面的优化实际上就是更加复杂的 BFS ，但是我是在阅读了社区的题解之后才意识到可以使用 BFS 实现。

BFS 也需要用到两个数据结构，VecDeque 和 HashMap ，其中队列中保存的是当前矩阵中邻接卷纸数量小于 4 的卷纸坐标，而 HashMap 则不断记录着每个卷纸的邻接卷纸数量。

每次从队列中弹出卷纸时，更新这个卷纸的邻接卷纸的邻接数量，如果数量小于 4 则将该邻接卷纸入队，依次进行直到队列为空。具体实现见代码 [code](https://github.com/livexia/advent-of-code-2025/blob/main/aoc04/src/main.rs)，第二部分运行时间差不多是 70ms ，这个 BFS 实际上和前面的优化是一致的，所以在时间上没有快很多，但是算法更加简洁实现也更加容易。

因为暴力的方法很简单，所以没有单列与代码中，因为很久没写代码了，所以对于 BFS 有所模糊，最终的代码之保留了 BFS 的实现，BFS 中使用到的 Hash 数据结构也可以通过 Vec 替代，因为输入较小这样也可以提高实际的运行效率，同时提及的运行时间都是 debug 下得到的，release 的运行时间会更少。

## Day 5

今天是区间题，输入分为两个部分，第一部分为区间列表，第二部分为现有 id 列表。第一部分的题目要求计算有多少 id 落在给定的区间中。第一部分简单的暴力循环遍历即可，同样也可以将区间列表合并然后排序，再根据区间起点对 id 进行二分搜索，以降低运行时间提高运行效率，见 [代码](https://github.com/livexia/advent-of-code-2025/commit/f3f498a089d29dda64e26aba6e07ada9a0ccf158)。

第二部分则需要计算所有区间列表中可能的所有 id ，当然不允许重复的 id 。因为给定的区间存在大量重叠的情况，而且由于实际的输入区间较多且较大，那么通过遍历所有区间可能的 id 就不现实了。那么就需要合并区间，而要合并所有区间的第一步就是合并两个区间。

### 合并两个区间

首先根据区间的起点排序并交换给定的两个区间 A 和 B，区间 A 为起点较小的区间，那么如果 A 的终点大于或等于区间 B 的终点，那么合并后的区间就是 A ，如果 A 的终点小于 B 的终点，那么合并后的区间就是 A 的起点到 B 的终点。

题目的**区间是两头都封闭的**，那么如果两个区间头尾相接，则这两个区间也可以合并。
代码

```rust
fn merge_range(r: IdRange, other: IdRange) -> Option<IdRange> {
    let (r, other) = if r.0 > other.0 {
        (other, r)
    } else {
        (r, other)
    };
    if r.1 + 1 < other.0 {
        None
    } else {
        Some((r.0, r.1.max(other.1)))
    }
}
```

### 合并所有的区间

~~我一开始不想使用笨方法，但是简单的几次尝试之后发现都不行，于是使用笨方法得出了答案。存在两个队列，队列 A 是等待合并的区间，队列 B 是已经合并的区间。每次从队 A 弹出一个区间 a，循环遍历队列 B 的区间 b，直到 a 能和 b 合并，将合并后的区间放入队列 B ，直到队列 A 为空。然后交换队列 A 和 B 再次循环，直到没有任何新的合并发生，此时所有的区间都完成了合并，第二部分的答案只需要计算区间的长度即可。~~

经过一番搜索之后，我发现了一个合并所有区间的[算法](https://stackoverflow.com/a/5276789)，我才意识到太久没写代码让我的思维有些慢了，因为明明是很简单算法，没道理我不知道的。

算法

1. 排序 (Sort): 首先按区间的起始点进行排序。
2. 合并 (Merge): 然后遍历排序后的列表。
3. 循环：
    1. 初始区间 current 为排序后的第一个区间
    2. 从排序后的第二个区间开始遍历区间 next。
    3. 如果 current 和 next 可以合并为区间 m，令 current = m 循环继续。
    4. 如果不能合并，则将 current 放入已经完成合并的区间数组，同时令 current = next 循环继续。
        - 具体说明：如果不能合并，则说明 current 和 next 不存在重叠，由于区间是有序的，那么包括 next 之后的所有区间都无法和 current 合并。即 current 已经是确定的完成合并的区间，可以不必再考虑是否能和其他区间合并。
    5. 循环结束后，需要将最后的 current 放入已经完成合并的区间数组，避免遗漏。

代码

```rust
fn merge_ranges(ranges: &[IdRange]) -> Vec<IdRange> {
    let mut ranges = ranges.to_vec();
    ranges.sort();
    let mut merged = vec![];

    let mut current = ranges[0];

    for &next in &ranges[1..] {
        if let Some(m) = merge_range(current, next) {
            current = m;
        } else {
            merged.push(current);
            current = next;
        }
    }
    merged.push(current);
    merged
}
```

## Day 6

今天的题目有点陷阱题的意思，陷阱主要在于输入的处理上，输入类似于表格的形式，看到这个输入立马按照行进行了处理，将一个个数字和运算符解析出来，最终形成两个数组，这样处理之后轻松的就能完成第一部分。


### 列解析与逆序计算

第二部分的核心挑战在于必须采用**逆序的、基于列**的解析方式，而非传统的行处理。为了高效实现这一目标，我们对输入数据进行了预处理，并将运算逻辑与数据累积分离。

#### 1. 数据预处理（逆序化）

* **目标:** 实现从右到左的列迭代。
* **方法:** 对输入文本的每一行（去除空行后），将其**字节序列逆序**。
    * 例如：原输入行 `4 123` 预处理后变为 `[3, 2, 1, ' ' , 4]` (字节表示)。
* **结果:** `lines: Vec<Vec<u8>>` 数组中的列索引 $i$ （从 0 开始）现在对应原始输入中的**从右往左**的列。

#### 2. 核心计算流程（逐列迭代）

代码从 $i=0$ 开始，逐列向左迭代，并在每个列上执行两个主要动作：**数字提取**和**操作符判断**。

##### A. 数字提取 (`real`)

对于当前列 $i$:

1.  **范围:** 仅查看**操作符行之上**的所有行（即除了最后一行）。
2.  **过滤:** 忽略当前列中的所有**空格** (`b' '`) 字符。
3.  **组合:** 将当前列中，从上到下遇到的所有数字字符**组合成一个多位数字** `real`。
    * **公式:** 采用 **`fold(0, |r, n| r * 10 + n)`** 实现，其中 $r$ 是累积结果，$n$ 是当前行数字。

##### B. 操作符判断与计算

检查**最后一行**（操作符行）在当前列 $i$ 上的字符，根据其类型执行相应的逻辑。

| 最后一行字符 | 提取的 `real` 值 | 动作描述 | 对 `ans` 的影响 | `reals` 数组变化 |
| :--- | :--- | :--- | :--- | :--- |
| `b'+'` | 任何值 | **加法运算**。 | $ans += \text{sum}(\text{reals}) + \text{real}$ | 清空 (`reals.clear()`) |
| `b'*'` | 任何值 | **乘法运算**。 | $ans += \text{product}(\text{reals}) \times \text{real}$ | 清空 (`reals.clear()`) |
| 任何其他字符 | $> 0$ | **数字累积**。 | 无 | 推入当前数字 (`reals.push(real)`) |
| 任何字符 | $= 0$ | **序列中断**。 | 无 | 清空 (`reals.clear()`) |

> **`reals` 数组**：用于临时存储在一系列非操作符列中提取到的数字，直到遇到操作符为止。

#### 3. 结果

通过这种逐列、逆序的解析和即时计算，有效地解决了第二部分要求的计算逻辑，避免了复杂的中间数据结构转换，实现了高效的算法。

第二部分代码

```rust
fn part2<T: AsRef<str>>(input: T) -> Result<usize> {
    let _start = Instant::now();

    let mut ans = 0;
    let lines: Vec<Vec<_>> = input
        .as_ref()
        .lines()
        .filter(|l| !l.trim().is_empty())
        .map(|l| l.bytes().rev().collect())
        .collect();
    let op_row = lines.len() - 1;
    let mut reals = vec![];

    for i in 0..lines[0].len() {
        let real = lines[0..op_row]
            .iter()
            .filter_map(|row| {
                if row[i] == b' ' {
                    None
                } else {
                    Some((row[i] - b'0') as usize)
                }
            })
            .fold(0, |r, n| r * 10 + n);
        match lines[op_row][i] {
            b'+' => ans += reals.iter().sum::<usize>() + real,
            b'*' => ans += reals.iter().product::<usize>() * real,
            _ => {
                if real == 0 {
                    reals.clear();
                } else {
                    reals.push(real);
                }
            }
        }
    }

    println!("part 2: {ans}");
    println!("> Time elapsed is: {:?}", _start.elapsed());
    Ok(ans)
}
```
## Day 7

今天的题目难度适中，核心在于高效地追踪光束（beams）的分裂与传播。解题策略经历了从通用图搜索到**高效的行级遍历**的演变。

### 1\. 初始解法：图搜索策略 (BFS / DFS)

初始阶段，问题被视为图搜索问题，目的是覆盖所有可能路径并计算其组合：

#### 1\. BFS 部分：路径查找与去重

对于第一部分，关键在于追踪光束（beams）的移动路径并避免重复计算。

  * 我们使用 **BFS** 来确定所有可能被光束到达的位置。
  * 在 BFS 过程中，必须记录光束**已经到过的位置**，以防止**重复分割**或陷入**死循环**。这相当于使用一个集合（Set/Visited Array）来避免对同一状态的重复探索。

#### 2\. DFS 部分：结果缓存与优化

对于第二部分，目标是计算从每个分割点能产生的**平行世界总数**。

  * 我们使用 **DFS** 来递归地计算从任意分割处开始的路径总数。
  * 为提高效率，必须引入**中间结果缓存**（**Memoization** / 动态规划）。我们记录从**任意一个分割处**最终可能分割出的平行世界总数，从而避免重复计算相同子问题。

### 2\. 数据结构的选择与优化

在数据结构的选择上，我们进行了以下权衡：

  * 最初采用 **HashMap** 来表示输入数据是为了简化边界值判断。
  * 通过分析题目约束（光束**不会触发左右边界**，且**只会向下移动**），我们发现输入实际上是一个**稀疏矩阵**且移动方向受限。
  * 因此，理论上使用**二维向量 (`Vec<Vec<T>>`)** 的效率将**显著高于** HashMap，能带来更好的**空间局部性**和更快的访问速度。
  * **最终结论：** 尽管进行了数据结构替换，实际运行效率提升不明显，说明性能瓶颈主要在于**实际的运算逻辑**而非数据结构本身。

### 3\. 🚀 最终优化：行级遍历与输入特性利用 (Part 1)

通过分析问题特性，我们发现第一部分无需使用复杂的 BFS 进行状态追踪。由于光束只向下传播，我们可以采用**按行顺序遍历**的方法，**维护当前行光束的状态**，从而实现更高效的线性时间复杂度。

#### 核心算法逻辑：

我们维护一个布尔型数组 `beams`，其中 `beams[j] = true` 表示在当前行，有一束光会落到第 $j$ 列。

1.  **初始化：** `beams` 数组根据第一行的起始光束进行初始化。
2.  **按行迭代：** 遍历输入网格的每一行。
3.  **处理分割：** 对于当前行中的每个分割器（例如 `'^'`），如果该列存在光束 (`beams[j] == true`)，则**销毁原光束**并**生成左右两侧的新光束**，同时增加分割计数。

#### 优化点：利用输入的周期性

通过观察输入数据和题目要求，我们发现一个关键的优化机会：

  * **周期性：** 所有的分割器 (`splitter`) 都以**隔行**的方式出现。
  * **逻辑推论：** 一束光束遇到分割器并分裂后，下一行必定是**空行**（即不存在分割器）。这意味着光束遇到分裂器后，**至少需要跳过一行**才能再次遇到下一个可能的分裂器。
  * **效果：** 这种周期性允许我们使用 `step_by(2)` 来跳过空行，从而将实际迭代次数减少近一半。

利用这一特性，程序运行时间从约 $1 \text{ms}$ **大幅降低**至约 $80 \mu \text{s}$，体现了算法优化带来的巨大性能提升。

#### 核心代码展示 (Rust):

```rust
fn part1(grid: &Grid) -> Result<usize> {
    let _start = Instant::now();

    let mut count = 0;
    // 初始化第一行光束状态
    let mut beams: Vec<_> = grid[0].iter().map(|c| c == &'S').collect();

    // 利用 step_by(2) 跳过不包含 splitter 的空行，进一步优化性能
    for row in grid.iter().step_by(2) {
        // 迭代当前行的每一列
        for j in 0..beams.len() {
            // 如果存在光束且当前位置是分裂器
            if beams[j] && row[j] == '^' {
                beams[j] = false;       // 销毁原光束
                beams[j - 1] = true;    // 生成左侧新光束（利用题目约束：不触发边界）
                beams[j + 1] = true;    // 生成右侧新光束（利用题目约束：不触发边界）
                count += 1;
            }
        }
    }

    println!("part 1: {count}");
    println!("> Time elapsed is: {:?}", _start.elapsed());
    Ok(count)
}
```

### 4. 第二部分：动态规划与路径计数优化

第二部分的目标是计算从起始点到终点，所有可能产生的**平行世界（时间线）的总数**。虽然问题本质上是路径计数，但最优解法是采用\*\*自顶向下的动态规划（DP）\*\*思想，利用行级遍历实现高效计算。

#### 1\. 核心思想：动态规划路径累加

我们不再使用递归 DFS 加缓存 (Memoization)，而是采用**迭代式 DP**，按行顺序计算：

  * **状态定义：** 我们维护一个数组 `timelines`，其中 `timelines[j]` 记录了光束到达当前处理行时，**第 $j$ 列累计了多少条不同的时间线（路径）**。
  * **转移方程（分叉）：** 当光束在第 $j$ 列遇到分叉器 (`^`) 时，该列的路径数 `timelines[j]` 会被全部转移到下一行对应的左右两列。

#### 2\. 算法实现与状态维护

我们使用两个数组在每一步迭代中进行状态转移：

  * **`timelines` (Prev):** 记录**当前行**每个位置的累计时间线数。
  * **`next` (Current):** 记录这些时间线转移后，在**下一行**每个位置累计的时间线数。

在遍历当前行时：

1.  如果第 $j$ 列存在时间线且遇到分叉器 (`^`)，则将 `timelines[j]` 的值完整地累加到 `next[j - 1]` 和 `next[j + 1]`。
2.  如果第 $j$ 列存在时间线但**未**遇到分叉器，则将其值直接累加到 `next[j]`（即时间线笔直向下）。
3.  遍历完成后，用 `next` 数组更新 `timelines`，进入下一行计算。

#### 3\. 代码实现与性能

此迭代方法结合了输入数据的**隔行周期性**（利用 `grid.iter().step_by(2)` 优化），实现了 $O(N \cdot M)$ 的线性时间复杂度，并且避免了递归的开销。

#### 核心代码展示 (Rust):

```rust
fn part2(grid: &Grid) -> Result<usize> {
    let _start = Instant::now();

    // 初始状态：第一行只有'S'处有一条时间线，其余为0。
    let mut timelines: Vec<_> = grid[0].iter().map(|c| (c == &'S') as usize).collect();

    // 按行遍历，利用 step_by(2) 跳过不含 splitter 的空行。
    for row in grid.iter().step_by(2) {
        // next 数组存储下一行的状态，初始化为0
        let mut next = vec![0; timelines.len()];

        // 遍历当前行的所有时间线
        for (j, current_count) in timelines.iter().enumerate().filter(|(_, c)| c > &&0) {
            // current_count 是当前 j 列累计的时间线总数
            if row[j] == '^' {
                // 遇到分叉器，将路径数分配到左右两列
                next[j - 1] += current_count;
                next[j + 1] += current_count;
            } else {
                // 没有分叉器，路径数直行向下
                next[j] += current_count;
            }
        }
        // 更新状态，进行下一轮迭代
        timelines = next;
    }
    // 最终结果是最后一层所有列的时间线总数之和
    let count = timelines.iter().sum();

    println!("part 2: {count}");
    println!("> Time elapsed is: {:?}", _start.elapsed());
    Ok(count)
}
```

### 5. 总结与展望

尽管今天的程序在理论上仍有极致优化的空间——例如在**第一部分**引入**位掩码 (Bitmask)** 以利用 CPU 并行计算，或者在**第二部分**利用 **HashMap** (或稀疏索引) 来进一步压缩稀疏的时间线数据——但整体而言，目前的实现已经在代码可读性与运行效率之间取得了极佳的平衡。

值得注意的是，即便是最初构想的 BFS 和 DFS 方案，在当前的数据规模下也能保持不错的响应速度，并未出现严重的性能瓶颈。总体来看，今天的题目难度适宜，更像是一场帮助大家找回状态的**算法热身**。

## Day 8

题目输入包含一系列三维坐标，要求将这些点两两连接，且连接顺序必须按照坐标间的\*\*欧几里得距离（直线距离）\*\*由小到大进行。经过初步分析，这是一个典型的图论问题，非常适合使用 **并查集 (Union-Find / Disjoint Set Union)** 数据结构来解决。

由于需要重温算法细节，我查阅了 [维基百科](https://zh.wikipedia.org/wiki/%E5%B9%B6%E6%9F%A5%E9%9B%86) 并参考实现了并查集。

### 解题思路

首先，我们需要对数据进行预处理：生成所有可能的节点对（边），并计算它们之间的直线距离。随后，将这些边按照距离进行**升序排序**。

在并查集的实现中，我们使用一个数组 `circuits` 来维护节点的父子关系，其中 `circuits[i]` 表示节点 `i` 的父节点。

#### 初步尝试与遇到的问题

  * **第一部分**：要求在连接了距离最近的 $N$ 对节点后，计算所有连通分量的大小，并求出最大的三个连通分量大小的乘积。
  * **第二部分**：要求持续连接节点对，直到**所有节点都属于同一个连通分量**。此时，计算刚刚连接的那两个节点的 $x$ 坐标乘积。

最初，我采用了一种较为基础的并查集实现。为了计算连通分量的大小，我在每次合并操作后（或特定时刻），都需要遍历所有节点，通过 `find` 操作找到根节点，再利用 `HashMap` 统计每个根节点下的子节点数量。

这种方法在第一部分尚可接受，但在第二部分中显得效率极低。因为第二部分需要频繁检测“是否所有节点连通”，每次连接后都进行全量遍历和统计是不可接受的。

#### 算法优化：维护连通分量大小

为了解决性能瓶颈，我参考维基百科的思路，对并查集进行了微小的但关键的修改：**在并查集内部直接维护连通分量的大小**。

除了 `circuits` 数组外，我们新增一个 `sizes` 数组。

  * `sizes[i]` 表示以节点 `i` 为根的连通分量的大小。
  * **注意**：只有当 `i` 是根节点（即 `circuits[i] == i`）时，`sizes[i]` 的值才是准确且有意义的。

**查询 (Find) 操作：**
查询操作主要用于路径压缩，不涉及分量大小的变化。具体的实现中，我使用了标准的递归路径压缩，让节点直接指向根节点，从而加速后续查询。

```rust
fn find(&mut self, i: usize) -> usize {
    if self.parent[i] != i {
        // 路径压缩：递归找到根节点并更新当前父节点
        self.parent[i] = self.find(self.parent[i]);
    }
    self.parent[i]
}
```

**合并 (Union) 操作：**

这是优化的核心。当两个不同的连通分量合并时，采用了 “按大小合并” (Union by Size) 的策略：比较两个集合的大小，始终将较小的集合合并到较大的集合中。这不仅能保持树的平衡，还能防止树的高度过高。

```rust
fn union(&mut self, i: usize, j: usize) -> bool {
    let root_i = self.find(i);
    let root_j = self.find(j);

    // 只有当两个节点不在同一个集合时才合并
    if root_i != root_j {
        // 按大小合并：小树挂大树
        if self.size[root_i] < self.size[root_j] {
            self.parent[root_i] = root_j;
            self.size[root_j] += self.size[root_i];
        } else {
            self.parent[root_j] = root_i;
            self.size[root_i] += self.size[root_j];
        }
        return true;
    }
    false
}
```

### 最终方案

引入 `sizes` 数组后，两部分的求解变得非常高效：

1.  **第一部分**：连接指定数量的边后，只需遍历所有节点 `i`。如果 `circuits[i] == i`（即 `i` 是根节点），则 `sizes[i]` 即为该连通分量的大小。收集这些大小并排序即可求解。
2.  **第二部分**：在每次执行 `union` 操作并更新 `sizes` 后，直接判断当前新根节点的大小 `sizes[j_root]` 是否等于总节点数。如果相等，说明所有节点已连通，直接输出结果并终止循环。

### 总结

1.  **性能瓶颈分析**：本题的实际性能瓶颈在于**生成节点对并按距离排序**。这需要 $O(E \log E)$ 的时间复杂度（其中 $E$ 是边的数量，对于 $N$ 个点，$E \approx N^2$）。相比之下，优化后的并查集操作几近线性时间 $O(E \alpha(N))$，效率非常高。Rust 中使用 sort_unstable 进行排序比 sort 快，实际也是如此。

2.  **并查集原理回顾**：并查集之所以能保证“查找到同一个根”，归纳为以下三点核心机制：
    1.  **树形结构**：每个集合本质上是一棵树，所有节点的边都指向上级（父节点）。
    2.  **唯一出口**：每棵树只有一个根节点，且根节点的特征是指向自己（自环）。
    3.  **传递性**：所有的合并操作都是通过连接两个集合的**根节点**完成的。这保证了无论树的结构如何变化，子节点顺着父节点指针向上，最终一定能连通到新的根节点。

3. 今天的解题过程更像是一场“按图索骥”。虽然我第一时间就明确了应当使用并查集算法，但由于久疏战阵，对具体实现的细节记忆模糊，最终还是辅助了维基百科才完成代码。严格来说，这并不能算作完全的自主解题。题目本身难度适中，且并查集也是我过往多次学习和实践过的经典算法。这次的卡顿直观地反映出我编码熟练度的下滑，警示我未来需要加强基础算法的手写训练，避免眼高手低。

## Day 9 待深化学习

### 1\. 输入处理与类型迭代

在最初处理输入数据时，数据看起来只是普通的坐标点。但在着手解决第二部分的几何计算时，我意识到基础的整型可能无法满足需求。

Part 2 涉及大量的\*\*叉积（Cross Product）\*\*运算，其计算公式 $(B_x - A_x)(P_y - A_y) - ...$ 极易在中间步骤产生数值膨胀。为了确保后续复杂的几何运算绝对安全，避免任何潜在的整数溢出，我回过头将坐标点的类型定义调整为了 `(i128, i128)`。

这种基于核心算法需求反推数据结构选型的策略，为后续构建鲁棒的几何判断奠定了基础。输入解析部分则利用了 Rust 简洁的迭代器链（`filter`, `map`, `split_once`）完成，专注于将原始文本高效转换为强类型的坐标列表。

### 2\. Part 1: 无约束的最大矩形

第一部分的任务是在点集中找到能构成的最大矩形面积。这里的“面积”定义有些特殊，类似于计算像素网格的包围盒大小，即：
$$Area = (|x_1 - x_2| + 1) \times (|y_1 - y_2| + 1)$$

由于没有任何几何约束（不需要判断矩形是否被遮挡或在边界内），我直接采用了双重循环遍历所有点对。虽然时间复杂度是 $O(N^2)$，但在数据量允许的情况下，这是最直观且不易出错的解法。

### 3\. Part 2: 多边形内的最大矩形与几何算法

#### 解题思路：经典几何组合拳

当进入第二部分时，面对“判断矩形是否在多边形内”的问题，我选择了计算几何中最经典、最稳健的方案：**射线法定性 + 跨立实验定界**。

这个方案将问题拆解为两个维度：

1.  **点在哪里？** 确保矩形的顶点在多边形内部。
2.  **边怎么走？** 确保矩形的边没有穿过多边形的边界。

#### A. 核心算法实现

**1. 判定点在多边形内：射线法 (Ray Casting)**
为了确保矩形的四个顶点合法，我使用了射线法。

  * **原理**：从待测点向单一方向发射射线，统计与多边形边界的交点数量。奇数内，偶数外。
  * **实现细节**：利用 **叉积 (Cross Product)** 判断点与线段的相对位置，替代了不稳定的线性插值除法。
  * **参考资料**：
      * [Point in polygon - Wikipedia](https://en.wikipedia.org/wiki/Point_in_polygon)
      * [Linear interpolation - Wikipedia](https://en.wikipedia.org/wiki/Linear_interpolation)

**2. 判定边相交：跨立实验 (Straddle Test)**
为了确保矩形没有“穿墙”而出，我对矩形的四条边与多边形的所有边进行了相交检测。

  * **原理**：利用叉积判断两条线段是否相互跨越（即线段 A 的端点在线段 B 两侧，且线段 B 的端点在线段 A 两侧）。
  * **数学公式**：
    $$Cross(A, B, P) = (B_x - A_x)(P_y - A_y) - (B_y - A_y)(P_x - A_x)$$
  * **参考资料**：
      * [Line segment intersection - Wikipedia](https://en.wikipedia.org/wiki/Line_segment_intersection)
      * [Cross product - Wikipedia](https://en.wikipedia.org/wiki/Cross_product%23Computational_geometry)

#### B. 性能优化：AABB 坐标限制

考虑到题目输入数据具有 **正交性（Orthogonal，仅含水平/垂直线）**，我在跨立实验的基础上引入了 **AABB（轴对齐包围盒）** 逻辑作为一种性能优化。

对于正交线段，判断相交不需要计算昂贵的乘法叉积，只需比较坐标范围：

  * 如果 $Edge_x$ 在 $Rect_{x\_range}$ 之间，且 $Rect_y$ 在 $Edge_{y\_range}$ 之间，则视为相交（穿过）。

这种方法在本质上与跨立实验等价，但在正交场景下 CPU 指令更少，速度更快。

#### C. 算法局限性与数据幸存者偏差 (Critical Limitations & Data Bias)

尽管我的代码成功通过了 Day 9 的测试并拿到了星星，但我必须深刻地指出：**这种解法的成功在很大程度上归功于输入数据的“仁慈”，而非算法本身的完备性。**

通过 **射线法（验证顶点）** 结合 **跨立实验/AABB（验证边）** 的确得出了今天的正解，但这掩盖了一个逻辑上的致命缺陷：对于非凸多边形（Non-convex Polygon），基于“边界+采样”的判定逻辑是不完备的。

**1. 盲区一：完美凹陷填充 (The Perfect Dent)**
如果多边形存在一个凹陷区域，而我们选取的矩形恰好**完美填满**了这个凹陷（即矩形边与多边形凹陷边完全重合，没有发生十字交叉），现有的逻辑会彻底失效：
* **射线法**：矩形四个顶点都在多边形边界上，判定为“合法”。
* **跨立实验/AABB**：由于矩形边与多边形边是**重合**（Coincident）而非**交叉**（Strict Intersection），判定为“未穿墙”。
* **结果**：算法会误判该矩形在多边形内部，但实际上它是在外部填补了空缺。

**2. 盲区二：中心点检测的伪安全 (The Fallacy of Center Point Check)**
为了修复上述问题，直觉上的方案是增加 **“矩形中心点检测”**。然而，这依然只是概率上的补救，无法解决复杂的拓扑结构。
* **反例场景：沙漏型陷阱 (The Hourglass Trap)**
    想象一个大矩形区域，左右两侧各有一个极深的凹陷向中心挤压，几乎相遇，只在中间留下一条狭窄的通道（像一个沙漏）。
    * 如果我们选取一个覆盖整个区域的大矩形：
    * **顶点**：都在边界上（合法）。
    * **边界**：与外轮廓重合，未发生穿插（合法）。
    * **中心点**：恰好落在中间狭窄的通道上（合法）。
    * **实际结果**：矩形的左右大部分区域都包含了多边形的**外部凹陷**，应为**非法**。
* **结论**：只要是基于有限点（顶点、中心点、随机点）的采样方案，在面对连续几何空间的复杂拓扑（如回环、复杂的孔洞组合）时，永远存在漏网之鱼。

**3. 最终结论**
今天的输入数据中**恰好不存在**这些病态的几何结构，才使得我的解法能够侥幸过关。
如果追求数学上的绝对完备性，必须放弃“边界判定”的思路，转向 **社区精华** 中提到的 **坐标离散化 + 泛洪填充 (Flood Fill)**。只有通过泛洪填充建立严格的拓扑内外关系，才能无视几何形状的复杂性，给出永远正确的答案。
### 4\. Rayon 并行计算优化

在解决几何算法问题后，我面临了严重的性能挑战。Part 2 需要对每一对点生成的矩形进行复杂的几何检测，计算量随着点数增加呈指数级上升。

作为一个 Rust 初学者，我不确定如何手动管理多线程。为了解决这个问题，我咨询了 AI 是否有简单的方法来加速循环。AI 推荐了 `rayon` 库。

虽然我对 `rayon` 内部具体的工作机制（如 Work-Stealing 线程池等）并不十分清楚，但它的使用体验非常惊艳：

  * **代码改动**：只需简单地将迭代器从 `.iter()` 改为 `.into_par_iter()`。
  * **效果**：程序自动利用了多核 CPU 资源，计算速度得到了数量级的提升。

这种“黑盒”式的优化体验让我印象深刻——在不深入了解并发编程复杂细节的情况下，依然能够通过成熟的社区库解决实际的工程瓶颈。配合内部循环中的算法剪枝（`if area <= local_largest`），最终成功在合理时间内跑出了结果。

### 5\. 社区精华 (Community Insights)

在完成题目后，我查阅了 Reddit 上的社区讨论，发现了一个**在正确性与效率上均优于我当前解法**的高阶方案体系，以及其他有价值的工程经验：

  * **终极方案：坐标离散化 + 泛洪填充 + 二维前缀和 (Coordinate Compression + Flood Fill + Summed Area Table)**
    这套组合拳将几何问题转化为了纯粹的数据结构问题，彻底规避了复杂的浮点/边界判断。

      * **步骤 1：离散化 (Shrink Coordinates)**：将稀疏的巨大坐标（如 `[1000, 2000, 12345]`）排序并映射为紧凑的索引（如 `[0, 1, 2]`）。加入额外的边界坐标以确保外围有一圈空地。
      * **步骤 2：泛洪填充 (Flood Fill)**：从外围对网格进行“注水”泛洪。被水淹没的区域标记为 0（外部/空洞），未被淹没的区域（红/绿地块）标记为 1。这从拓扑上完美解决了所有凹陷和空洞问题。
      * **步骤 3：二维前缀和 (Summed Area Table)**：这是极速验证的关键。基于标记为 1 的内部地块构建 **Summed Area Table**。
          * 对于任意一对点构成的候选矩形，利用前缀和在 $O(1)$ 时间内计算该矩形区域内的数值总和。
          * **判定逻辑**：如果 `前缀和计算出的总值 == 矩形期望面积 (width * height)`，说明矩形内全是有效地块，没有空洞，合法；否则非法。
      * **来源**：[Rust Solution by u/maneatingape](https://www.reddit.com/r/adventofcode/comments/1phywvn/comment/nt2oahn/)

  * **并行暴力解法的有效性**
    有其他 Rust 开发者也采用了类似的策略：生成所有矩形并测试所有线段。

      * **性能**：该用户报告称，配合 `rayon` 并行化后，Part 2 的运行时间仅为 **6.1ms**。这验证了我选择 `rayon` 进行“暴力优化”不仅是可行的，而且在现代硬件上是非常高效的。
      * **来源**：[Rust Solution by u/Sad_Listen_4414](https://www.reddit.com/r/adventofcode/comments/1phywvn/comment/nt2ly9q/)

  * **常见的面积计算陷阱**
    社区中还讨论了一个容易忽视的 Bug：面积计算时的优先级问题。

      * **错误**：`abs(p1 - p2 + 1)` —— 先加 1 后取绝对值，可能导致结果错误。
      * **正确**：`abs(p1 - p2) + 1` —— 先取距离绝对值，再加 1（包含边界）。
      * **来源**：[Python Analysis by u/johnpeters42](https://www.reddit.com/r/adventofcode/comments/1phywvn/comment/nt2mk9f/)

### 6\. 总结与回顾 (Summary & Retrospective)

回顾今天的挑战，这不仅是一次编程练习，更是一场关于几何算法和工程优化的综合实战。

  * **核心算法决策 (Core Algorithms)**:
    本题我采用了 **射线法 + 跨立实验** 的正统几何解法。这是一种符合直觉且易于实现的方案，但在处理“完美凹陷”等极端拓扑结构时存在盲区。这让我意识到几何计算中“边界判定”与“区域判定”的本质区别。

  * **未来的学习方向 (Future Outlook)**:
    通过对比社区的高分回答，我发现 **“坐标离散化 + 泛洪填充 + 单调栈”** 才是此类问题的“版本答案”。

      * **正确性**：它通过泛洪填充建立了严格的拓扑内外关系，无视几何形状的复杂性（彻底消灭 L型陷阱、C型空洞）。
      * **效率**：它将 $O(N^3)$ 的暴力枚举优化为了接近线性或 $O(N^2)$ 的网格扫描，效率有质的飞跃。
        尽管这次我使用了 Rayon 暴力破解了 Part 2，但这套高阶算法逻辑严密且优雅，是我未来深入学习计算几何时的重点目标。

  * **Rust 工程化**:
    从 `i128` 的防溢出选择，到 `Rayon` 的并行加速，展示了 Rust 在处理计算密集型任务时强大的语言表现力和生态支持。

# Day 10 待深化学习

## 1\. Part 1: 直觉、实现与优化

题目入手时，第一部分要求通过按钮控制灯光的开关。面对有限的灯光数量和按钮组合，我的第一反应是使用搜索算法。

### 1.1 初始分析：位运算的引入

观察题目，灯的状态只有“开”和“关”，按钮的操作表现出明显的 **异或 (XOR)** 性质：

  * 按下 1 次：状态翻转。
  * 按下 2 次：状态还原。

这意味着每个按钮只有“按”和“不按”两种有效状态。为了追求性能，我决定放弃 `Vec<bool>`，转而使用 **Rust 的 `u128`** 进行状态压缩。位运算 (`^`) 使得状态转移极其高效。

### 1.2 BFS 实现中的关键调整

在使用 BFS 求解最短按键次数的过程中，我对去重逻辑进行了一次修正。虽然 Part 1 的数据规模较小，即使不优化也能通过，但这次调整让代码逻辑更加符合 BFS 的本质。

  * **调整前的思路**：
    最初为了防止死循环，我记录的是“按下了哪些按钮”的组合 (`button_pressed`)。但这种方式忽略了一个事实：不同的按钮序列可能会导致相同的灯光结果（殊途同归）。

  * **调整后的策略**：
    我将 `visited` 集合的去重对象改为 **“当前灯的状态” (current\_lights)**。

      * **逻辑收益**：只要当前的灯光 pattern 之前出现过，根据 BFS 的层序性质，当前路径一定不比之前的路径更优，可以直接剪枝。
      * **及早检查 (Eager Check)**：在生成新状态入队前就进行查重，保证了队列中始终只存储有效的、未处理过的状态。

**核心代码实现：**

```rust
fn min_presses_for_lights(&self) -> Option<usize> {
    let mut queue = VecDeque::new();
    let mut visited = HashSet::new(); // 记录灯的状态(u128)而非按钮组合

    queue.push_back((self.lights, 0, 0));
    visited.insert(self.lights); // 初始状态标记为已访问

    while let Some((current_lights, button_pressed, presses)) = queue.pop_front() {
        if current_lights == 0 {
            return Some(presses);
        }
        
        for (index, button) in self.buttons.iter().enumerate() {
            // 防止重复按同一个按钮（根据位掩码判断）
            if button_pressed & 1 << index == 1 { continue; }

            let next_lights = press_button(current_lights, button);
            
            // [核心优化] Eager Check: 在入队前检查状态是否已存在
            // 如果 next_lights 之前出现过，insert 返回 false，直接跳过
            if visited.insert(next_lights) {
                // 只有全新的状态才会被推入队列
                queue.push_back((next_lights, button_pressed | 1 << index, presses + 1));
            }
        }
    }
    None
}
```

虽然在当前的小规模输入下，这一改动带来的性能提升在感官上并不明显，但它消除了对同一灯光状态的重复计算，使得算法在逻辑上更加严谨和清晰。


## 2\. Part 2: 遭遇瓶颈与思维跃迁

进入第二部分，题目参数发生了剧烈变化：运算从 XOR 变为数值累加，且目标电压数值巨大。

### 2.1 尝试与失败：BFS/DFS 的局限

起初，我尝试沿用 Part 1 的思路，对代码进行了简单的修改，分别尝试了 **BFS** 和 **DFS** 来寻找解。
然而，在长时间的运行等待后，程序始终无法计算出结果，这迫使我不得不重新审视问题：

  * **搜索空间无限**：由于不再有“按两次抵消”的性质，且没有明确的上限，状态空间从有限的 $2^N$ 变成了无限的整数域。
  * **深度过深**：目标值极大意味着需要按键的次数 `times` 可能是天文数字，递归或队列瞬间就会爆栈或耗尽内存。

在搜索算法卡住并确认失效后，我意识到必须跳出模拟的思维，寻找数学层面的解析解。

### 2.2 深度建模：从物理连接到线性方程组

既然是求具体的按键次数，我们可以将问题抽象为数学模型。
设 $x_j$ 为第 $j$ 个按钮被按下的次数，这构成了未知的变量向量 $x = [x_0, x_1, \dots, x_{n-1}]^T$。

#### 构造矩阵 ($A$)

根据题目逻辑，每个按钮会影响特定几根电线（维度）的电压。我们可以构建一个 **0/1 关系矩阵** $A$：

  * 行 ($i$)：代表第 $i$ 个维度的电压（Wire $i$）。
  * 列 ($j$)：代表第 $j$ 个按钮（Button $j$）。
  * 元素 $A_{ij}$：如果 Button $j$ 连接到 Wire $i$，则 $A_{ij} = 1$，否则为 $0$。

#### 构造方程 ($Ax=B$)

对于每一个电压维度 $i$，其最终电压等于所有连接该维度的按钮按下次数之和。于是得到方程组：

$$
\begin{cases}
A_{0,0}x_0 + A_{0,1}x_1 + \dots + A_{0,n-1}x_{n-1} = \text{Target}_0 \\
A_{1,0}x_0 + A_{1,1}x_1 + \dots + A_{1,n-1}x_{n-1} = \text{Target}_1 \\
\dots \\
A_{m-1,0}x_0 + A_{m-1,1}x_1 + \dots + A_{m-1,n-1}x_{n-1} = \text{Target}_{m-1}
\end{cases}
$$

#### 核心难点：不定方程组

观察矩阵的规模，我发现 **变量的数量 (按钮数 $N$) 往往多于方程的数量 (电压维度 $M$)**。
在数学上，这是典型的 **不定方程组 (Underdetermined System)**。这意味着可能存在无穷多组实数解，而我们需要从中找出满足 **$x_i$ 必须是非负整数** 且 **$\sum x_i$ 最小** 的那一个。

### 2.3 解决方案：混合整数线性规划 (MILP)

面对这个标准的优化问题，我选择了 **Rust 的 `good_lp` 库** 进行求解。

  * **实现细节**：
      * **变量**：定义 $N$ 个整型变量，下界为 0 ($x_i \in \mathbb{Z}_{\ge 0}$)。
      * **约束**：将上述线性方程组逐行转化为 Solver 的约束条件 `expression.eq(target)`。
      * **目标**：设置目标函数为 Minimize $\sum x_i$。

**核心代码实现：**

```rust
fn min_presses_for_joltage_good_lp(&self) -> Option<usize> {
    let f_count = self.joltage.len();
    let b_count = self.buttons.len();

    // 1. 构建系数矩阵 (0/1 Matrix)
    let mut f = vec![vec![0; b_count]; f_count];
    for (index, button) in self.buttons.iter().enumerate() {
        for &b in button {
            f[b][index] = 1; // Button index 影响 Wire b
        }
    }

    // 2. 定义问题变量
    let mut problem = variables!();
    // 约束：每个按钮次数 >= 0 且为整数 (Integer)
    let vars = vec![variable().min(0).integer(); b_count]; 
    let t: Vec<Variable> = problem.add_all(vars);

    // 3. 定义目标函数：最小化总按键次数
    let objective: Expression = t.iter().sum();
    
    // 初始化模型
    let mut model = problem.minimise(&objective).using(default_solver);
    // model.set_parameter("verbose", "false"); // (注：实际测试中并未完全屏蔽 CBC 输出)

    // 4. 添加线性约束 Ax = B
    for (row, &j) in f.iter().zip(&self.joltage) {
        let mut constraint: Expression = Expression::from(0);
        for (&coeff, &var) in row.iter().zip(&t) {
            if coeff == 1 {
                constraint += var;
            }
        }
        // 添加约束：sum(affected_buttons) == target_joltage
        model = model.with(constraint.eq(j as f64))
    }

    // 5. 求解并取整
    match model.solve() {
        Ok(sol) => Some(sol.eval(objective).round() as usize),
        Err(e) => {
            println!("Solver error: {e:?}");
            None
        }
    }
}
```

  * **工程依赖与问题**：
    在实际使用中，`good_lp` 默认调用 **CBC Solver** 作为后端求解器。

      * **环境要求**：这并非纯 Rust 实现，电脑需要预先安装 CBC (如 `brew install cbc`) 才能编译运行。
      * **日志干扰**：CBC 求解器在运行时会向标准输出打印大量的迭代日志信息。虽然尝试配置参数进行隐藏，但在当前环境中并未生效，输出中包含了不少 CBC 的原生输出。

  * **替代思考**：
    除了 `good_lp`，往年 Advent of Code 中常用的 **Z3 求解器** 也是一个极佳的替代方案，它同样擅长处理此类数学约束问题。

最终，尽管伴随着大量的求解器日志，程序成功计算出了 Part 2 的结果。


## 3\. 复盘：理论视角的缺失与反思

回顾今天的解题过程，虽然最终通过工程手段拿到了星星，但在算法的底层理解和实现上，暴露出了明显的短板。

### 3.1 “调用库”与“算法实现”的差距

在 Part 2 中，我虽然成功识别出了 **线性方程组** 的数学模型，但在求解环节，我选择了依赖第三方的 Solver (`good_lp`)。

  * **本质分析**：这实际上是一种“外包”策略。我并没有真正编写解决不定方程组的算法代码。
  * **算法缺失**：如果脱离了 `good_lp` 或 `z3` 这样的黑盒工具，我应当能够手写 **高斯消元法 (Gaussian Elimination)** 来解决这个问题。
  * **统一视角**：事后反思发现，Part 1 和 Part 2 本质上都是线性代数问题。Part 1 是 **有限域 GF(2)** 上的高斯消元（异或消元），Part 2 是实数/整数域上的高斯消元。没能亲手实现这一核心算法，是本次练习最大的遗憾。

### 3.2 备选视野：折中搜索 (Meet-in-the-middle)

在查阅资料时，我还了解到一种名为 **双向搜索/折中搜索** 的策略。
虽然本次没有深入实践，但它作为一种经典的“空间换时间”算法，在处理中等规模（如按钮数 $N=40$）且无法直接数学求解的问题时，往往是最佳方案。这点值得记录，作为未来的备选工具库。


## 4\. 后续学习与练习计划 (Action Plan)

鉴于本次解题在 Part 2 中过度依赖工具库，我制定了以下强化计划，旨在补全从“工程调用”到“算法落地”的最后一块拼图。

### 4.1 线性代数基础巩固

要真正解决此类问题，必须跳出“套公式”的层面，深入理解背后的数学结构：

  * **矩阵的秩 (Rank) 与解的结构**：深入理解为什么不定方程组会有无穷多解，以及如何通过自由变量和主元变量来表达通解（General Solution）。
  * **有限域运算**：系统学习 GF(2) 上的矩阵运算，搞懂为何异或方程组的消元过程与实数域完全一致，只是加法变成了异或。

### 4.2 核心算法的手动实现 (Wheel Reinvention)

为了摆脱对 Solver 黑盒的依赖，我计划在接下来的练习中，尝试**不使用任何第三方数学库**，手动实现以下算法：

1.  **高斯消元法 (Gaussian Elimination)**：
      * 实现一个通用的消元器，能够将任意矩阵化为**行阶梯形 (Row Echelon Form)**。
      * 能够判断无解、唯一解和无穷多解的情况。
2.  **异或高斯消元 (XOR Gaussian)**：
      * 针对 Part 1 类型的题目，实现基于 `BitSet` 优化的消元算法。
3.  **基础单纯形法 (Simplex Method)**：
      * 作为长期目标，尝试理解并实现最基础的单纯形法，以理解 `good_lp` 这类线性规划库背后的运行机制。

### 4.3 总结

今天的练习是一次“工程优先”的生存训练：

1.  **独立解决**：在 Part 1 中通过修正 BFS 去重逻辑，独立实现了清晰可靠的解法。
2.  **受阻转向**：在 Part 2 搜索失效后，及时止损，转向数学建模。
3.  **未来方向**：从调用 API 转向掌握核心算法，补全线性代数与数值计算的知识盲区。

## Day 11 

### 1\. 初步分析与起步

**题目目标**：解析设备连接关系（格式为 `id: out1 out2 ...`），构建有向图，并计算从起点到终点的路径数量。

**初始技术选型**：
为了快速验证逻辑，起初采用了最直观的哈希表实现：

1.  **图存储**：使用 `HashMap<usize, Vec<usize>>` 存储邻接表。虽然存在哈希计算开销，但无需关心节点 ID 的稀疏性和插入顺序。
2.  **ID 映射**：使用 `HashMap<String, usize>` 将字符串转为数字 ID。

### 2\. Part 1: 基础路径计数 (DFS + HashMap 缓存)

第一部分是标准的路径计数问题（从 `you` 到 `out`）。

### 算法逻辑

采用**记忆化深度优先搜索 (Memoized DFS)**。

1.  **基准情况**：如果当前节点 `current` 等于目标 `target`，返回 1。
2.  **缓存检查**：检查 `cache` 是否已存在当前节点的计算结果。
3.  **递归推进**：遍历当前节点的所有输出节点 `next`，递归调用并将结果累加。

#### 核心代码

```rust
fn count_paths_dfs(
    current: usize,
    target: usize,
    connections: &Connections,
    cache: &mut HashMap<usize, usize>, // 初始版本使用 HashMap
) -> usize {
    if current == target {
        return 1;
    }
    if let Some(count) = cache.get(&current) {
        return *count;
    }

    let mut count = 0;
    for &next in connections.get_outputs(current) {
        count += count_paths_dfs(next, target, connections, cache);
    }
    cache.insert(current, count);
    count
}
```

### 3\. Part 2: 引入约束与算法演进

第二部分要求路径必须经过 `dac` 和 `fft` 两个特定节点。对此我探索了两种解法。

#### 方法一：状态压缩 DFS (State Compression)

在 DFS 中引入位掩码 `visited_mask` 来追踪必经点的访问状态。

**算法逻辑**：

  * **状态定义**：`(current, mask)`。`mask` 的第 0 位代表 `dac`，第 1 位代表 `fft`。
  * **状态转移**：在遍历子节点时，使用位运算 `|` 更新掩码。
  * **有效判定**：只有当 `mask == 3` (二进制 `11`) 且到达终点时，才返回 1。

**核心代码**：

```rust
fn count_paths_with_dac_fft(
    current: usize,
    target: usize,
    visited_mask: u8,
    dac_fft: &[usize],
    connections: &Connections,
    cache: &mut HashMap<(usize, u8), usize>,
) -> usize {
    // 只有经过了所有必经点 (mask == 3) 到达终点才算数
    if visited_mask == 3 && current == target {
        return 1;
    }
    if let Some(count) = cache.get(&(current, visited_mask)) {
        return *count;
    }

    let mut count = 0;
    for &next in connections.get_outputs(current) {
        let next_mask = visited_mask
            | if next == dac_fft[0] { 1 } else if next == dac_fft[1] { 2 } else { 0 };
        count += count_paths_with_dac_fft(next, target, next_mask, dac_fft, connections, cache);
    }
    cache.insert((current, visited_mask), count);
    count
}
```

#### 方法二：分段路径计数 (Segmented Path Counting)

利用路径的拓扑特性将长路径拆分。路径只可能是 `svr->dac->fft->out` 或 `svr->fft->dac->out`。

**算法逻辑**：

1.  **路径分解**：复用 Part 1 的通用 DFS 函数，分别计算每段路径的数量。
2.  **关键实现**：定义闭包 `count_between` 来封装带缓存初始化的 DFS 调用。
3.  **累加逻辑**：分别计算“先过 dac”和“先过 fft”两种情况，并将结果**累加**。

**核心代码**：

```rust
// 关键闭包：封装每次独立的路径计算，确保缓存隔离
let count_between = |start, end| {
    // 每次调用都初始化一个新的缓存
    count_paths_dfs(start, end, connections, &mut HashMap::new())
};

let mut count = 0;

// 情况 1: svr -> dac -> fft -> out
let dac_fft = count_between(dac, fft);
if dac_fft != 0 {
    let svr_dac = count_between(svr, dac);
    let fft_out = count_between(fft, out);
    count += svr_dac * dac_fft * fft_out;
}

// 情况 2: svr -> fft -> dac -> out
let fft_dac = count_between(fft, dac);
if fft_dac != 0 {
    let svr_fft = count_between(svr, fft);
    let dac_out = count_between(dac, out);
    count += svr_fft * fft_dac * dac_out; // 修正：使用 += 累加
}
```

#### 复杂度对比分析

假设图顶点数为 $V$，边数为 $E$，必经点数量为 $K=2$。

| 维度 | 方法一：状态压缩 (State DFS) | 方法二：分段计数 (Segmented) |
| :--- | :--- | :--- |
| **时间复杂度** | $O(2^K \cdot (V+E))$ | $O(K! \cdot (V+E))$ |
| **本题情况 ($K=2$)** | $O(4 \cdot (V+E))$ | $O(6 \cdot (V+E))$ |
| **空间复杂度** | $O(2^K \cdot V)$ (缓存所有状态) | $O(V)$ (每次仅缓存当前段) |

**实际运行效率总结**：
尽管两种方法在理论常数上略有差异（4 vs 6），但由于本题中必经点数量极少 ($K=2$)，这部分的计算开销相对于图遍历本身是非常小的。**在实际代码运行中，两种方法的耗时几乎一致**，都能够快速求解。

### 4\. 重构：利用数组操作替换哈希操作

在逻辑跑通后，为了追求运行效率，我对底层数据结构进行了全面重构，利用 `Vec` 的索引操作全面替换了 `HashMap` 的哈希操作。

#### 优化一：图存储结构的 Vec 化

考虑到设备 ID 是从 0 开始连续分配的，`Vec` 的索引访问远优于哈希查找。

  * **结构变更**：

      * 旧：`adj_list: HashMap<usize, Vec<usize>>`
      * 新：`adj_list: Vec<Vec<usize>>`

  * **实现细节**：
    `Vec` 需要管理容量。在 `add_connection` 中加入了自动扩容逻辑：

    ```rust
    if input_id >= self.adj_list.len() {
        self.adj_list.resize(input_id + 1, vec![]);
    }
    ```

#### 优化二：缓存结构的 Vec 化

这一步优化的核心在于**可行性分析**。

  * **可行性分析**：
    在预处理阶段，我们采用了“自增整数”的方式分配设备 ID（0, 1, 2... N）。这意味着所有的状态标识符都是**连续且紧凑**的整数。这种数据特性完美契合数组（Vec）的内存布局，使得我们可以直接使用 ID 作为下标，而不会造成内存浪费（即不存在稀疏数组问题）。

  * **Part 1 缓存优化**：
    将 `HashMap<usize, usize>` 替换为 `Vec<Option<usize>>`。

  * **Part 2 状态缓存优化 (Flat Map)**：
    虽然状态是二维的 `(node, mask)`，但 `mask` 取值范围固定为 $0-3$。我们可以利用这一特性进行扁平化映射。

      * **映射公式**：`index = node * 4 + mask`
      * **优势**：将复合键的哈希查找转换为极速的整数乘加运算。

#### 数据结构性能对比

| 特性 | HashMap (优化前) | Vec (优化后) |
| :--- | :--- | :--- |
| **查找复杂度** | 平均 $O(1)$，最差 $O(N)$ | 严格 $O(1)$ |
| **常数开销** | **高** (哈希计算 + 解决冲突) | **极低** (仅内存偏移) |
| **内存布局** | 分散 (Cache Locality 差) | 连续紧凑 (Cache Locality 好) |
| **实现复杂度** | 低 (自动处理稀疏 ID) | 中 (需手动处理 Resize) |

**优化总结**：虽然 `Vec` 需要额外处理扩容和边界检查，但消除哈希开销带来的性能收益是巨大的，特别是对于需要数百万次递归调用的 DFS 算法而言。

### 5\. 总结与反思

#### 初见题目的“环路担忧”

在刚看到这道题时，我的第一反应是输入数据中可能包含环路。

  * **最初的设想**：Part 1 可能因为运气好避开了环，但 Part 2 强制经过特定点，可能会陷入环路陷阱。我甚至预想可能需要先进行“破环”（移除某条连接）预处理，再计算路径。
  * **最终的验证**：事实证明这份担忧是多余的。代码在没有环路检测机制的情况下顺利运行且无栈溢出，这反向验证了题目隐含了一个关键约束——输入数据是一个**有向无环图 (DAG)**。

#### 方法论复盘：为何直觉指向了 DFS？

在选择算法时，直觉引导我选择了 DFS 而非 BFS。这并非巧合，而是由本题“路径计数”和“状态约束”的特性决定的：

1.  **问题语义的匹配 (Path vs Layer)**：
      * **DFS** 是面向**路径**的搜索。它的执行过程本身就是一条从起点扎向终点的完整路径，这与题目“寻找有多少条路径”的语义完全一致。
      * **BFS** 是面向**层级**的搜索。它擅长处理“最短路径”问题。若要用于计数，必须显式地进行**拓扑排序**（计算入度、等待依赖就绪），否则无法正确累加路径数。在 DFS 中，递归的返回顺序天然就是拓扑序，省去了这一步骤。
2.  **状态管理的成本 (Implicit vs Explicit)**：
      * 在 Part 2 中，我们需要维护 `visited_mask`。
      * **DFS**：`mask` 仅仅是递归函数的一个参数。递归栈自动帮我们保存了每一层的状态，当递归返回（回溯）时，状态自动恢复。
      * **BFS**：我们需要在队列中显式存储 `(node, mask)` 元组。这不仅增加了内存开销，还需要在入队/出队时手动管理状态的流转和分支。

#### 技术收获

今天的题目虽然难度被归为“放松题”，但它是一个绝佳的工程化演练场。

  * **算法维度**：从通用的“状态压缩”到利用拓扑特性的“分段计数”，展示了针对 DAG 特性寻找更优解的思路。
  * **数据维度**：从“能用”的哈希表到“极致”的数组操作，展示了在数据紧凑连续的前提下，如何通过底层内存优化来压榨程序的极限性能。
