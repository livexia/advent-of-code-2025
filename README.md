# Advent of Code 2025

- https://adventofcode.com/
- https://github.com/livexia/advent-of-code-2025

## Day 1

今天的问题算是轻松，输入的每一行决定旋钮的转向和旋转的距离，输入的处理也不复杂，但是要注意在解析数字时的错误处理，输入处理完成后，如果旋钮是向左旋转则距离为负数，向右则为正数。

第一部分只需要计算旋转之后的刻度位置是否为 0 即可，可以简单的求 100 的余数即可。

第二部分稍微复杂一些，需要计算每次旋转过程中经过了多少次 0 刻度位置，给出的测试用例较小，实际的输入旋转的距离可能是多圈的，要考虑到这一点。首先计算每一次旋转的距离至少有多少圈，每转一圈密码加一。接着计算整圈之外的旋转距离，如果起始点不在 0 ，那么旋转结束后，如果刻度跨过 0 刻度，密码加一。因为向左旋转距离是负数，如果当前位置不为 0 ，同时当前位置加上旋转距离为负数，则旋钮指针一定向左跨过一次 0 。同理如果当前位置不为 0 （不为 100），同时向右旋转，而旋转后的刻度数超过 100 ，那么指针一定向右旋转过一次 0。根据这个逻辑对 password 进行加一即可。

第二部分代码

```rust
fn part2(rotations: &[i32]) -> Result<u32> {
    let _start = Instant::now();

    let mut dial = 50;
    let mut password = 0;

    for rot in rotations {
        // 计算一定会经过 0 刻度的整圈次数
        password += rot.unsigned_abs() / 100;

        // 扣除整圈的旋转距离，
        // 可以规避例如从 0 刻度旋转距离 100 的边界情况
        let rot = rot % 100;

        let temp = dial + rot;
        // 当起点不是 0 刻度时，
        // 向左或向右旋转超过或位于刻度边界 0 或 100 时，
        // 一定经过一次 0 刻度
        password += (dial != 0 && (temp >= 100 || temp <= 0)) as u32;

        // 计算旋转后刻度的真实位置，刻度值一定大于 0
        dial = temp.rem_euclid(100);
    }

    println!("part2: {password}");
    println!("> Time elapsed is: {:?}", _start.elapsed());
    Ok(password)
}
```

## Day 2

今天要求找出区间内存在重复模式的数字，第一部分要求找出左右两个部分相同的数字，第二部分要求找出数字经过 N 等分后，每个部分都相同的数字。第二部分其实是第一部分的衍生，解题思路在两个部分是一致的。我首先利用暴力法得出题解，通过取余的方法不断的分割数字，对比分割的结果，即可确定是否存在重复模式。暴力法效率不高，因为我遍历了区间内的所有数字，依次检查数字，这样实际效率很慢。

### 渐进寻找

暴力法中区间内的数字是依次递增的，但是应该有更加高效的方法确定符合重复模式的数字。考虑区间 565653-565659 ，可见这个区间中所有的数字都是 6 位等长的，我们先考虑第一部分的重复模式，即数字的前半部分和后半部分应当相同。考虑区间起点 565653 ，直接将区间按照重复模式进行分割，可得到两个数字 565 和 653 ，同样的将结尾也进行分割得到 565 和 659 ，可见两个数字的前半部分相同，那么这个部分就不能变动，那么 653 和 659 就应该变化为 565 ，得到的数字是 565565 不在区间内。所以按照第一部分的匹配模式，无法从区间内寻找到符合的数字。那么考虑第二部分的匹配模式，这个区间的数字长度都为 6 ，那么存在长度为 1、2 和 3 的三种分割模式。确定区间起点和结尾数字中，共同的部分为 56565，那么根据这个共同部分进行分割，可以发现长度 1 或 3 的分割模式是不可能的。考虑分割长度为 2 ，那么分割后每个部分都需要是 56 才行，同时 56 刚好落入起点和结尾 53 - 59 之间，那么长度为 2 的分割可行。

这个方法存在一种情况，那就是如果区间的数字长度不一致，比如区间 95-115 就不容易寻找了，当然可以把区间进行拆分，95-115 变成 95-99 和 100-115 两个区间，分开寻找即可。

**效率对比**

```
part1: 26255179562
> Time elapsed is: 22.953333ms
part2: 31680313976
> Time elapsed is: 37.9495ms
part1 by step: 26255179562
> Time elapsed is: 58.208µs
part2 by step: 31680313976
> Time elapsed is: 70.834µs
```

渐进查找主要代码
```rust
fn split_range(start: usize, end: usize) -> Vec<(usize, usize)> {
    let (start_l, end_l) = (start.ilog10(), end.ilog10());
    if start_l < end_l {
        let mut ranges = vec![];
        let mut start = start;
        for i in start_l..=end_l {
            let new_end = 10usize.pow(i + 1) - 1;
            ranges.push((start, new_end.min(end)));
            start = new_end + 1;
        }
        ranges
    } else {
        vec![(start, end)]
    }
}

fn find_invalid(start: usize, end: usize, base: u32) -> Vec<usize> {
    assert_eq!(start.ilog10(), end.ilog10());
    let l = start.ilog10() + 1;
    if !l.is_multiple_of(base) {
        return vec![];
    }
    let (start_left, end_left) = (start / 10usize.pow(l - base), end / 10usize.pow(l - base));
    let mut invalids = Vec::new();
    for s in start_left..=end_left {
        let n = (0..l)
            .step_by(base as usize)
            .fold(0, |n, i| n + s * 10usize.pow(i));
        if start <= n && n <= end {
            invalids.push(n);
        }
    }
    invalids
}
```

## Day 3

今天要求计算一个数字序列的最大子串（数字），第一部分限定子串长度为2，第二部分则限定长度为12，同时子串的顺序不变。输入的处理不复杂，思路也很简单，就是遍历数字序列，依次寻找最大值即可。

考虑数字序列 96781 ，需要寻找长度为 2 的最大子串，假设子串为 ab ，那么优先从给定序列中确定最大的 a ，同时确保能找到 b 即可。搜索 a 时从左到右进行搜索，而搜索到最后一个数字之前就需要停止，也就是搜索 9678 即可。在搜索 a 的过程中不必考虑是否可能会导致 b 的值不是最大，因为最后的要求的是 ab 最大即可。假设搜索过程中为了确保 b 的值为最大的 d ，而导致 a 取得了较小的值 c，即 c < a 且 d > b ，那么 10 * c + d 一定小于 10 * a + b 。

两个部分对于子串的长度要求不同，但是思路是一致的。从左到右依次搜索字串最大值的过程中，需要记录第一次遇到最大值的元素位置，而非其他可能遇到最大值的位置，这样可以避免影响后续元素最大值搜索。参考序列 98975，需要寻找长度为 2 的最大子串，a 确定搜索到的最大值为 9，如果 a 记录的最大值位置不为 0 而为 2 ，那么搜索 b 时就会从位置 3 开始搜索，最后得到子串为 97 ，是错误结果。

贪心算法核心代码

```rust
fn find_largest_joltage(battery: &[usize], number: usize) -> usize {
    let length = battery.len();
    let mut joltage = 0;
    let mut next_battery = 0;
    for l in (0..number).rev() {
        let mut max_battery = 0;
        (next_battery..(length - l)).for_each(|left| {
            if battery[left] > max_battery {
                max_battery = battery[left];
                next_battery = left + 1;
            }
        });
        joltage = joltage * 10 + max_battery;
    }
    joltage
}
```

这个算法还可以进行剪枝，因为输入的数字序列限制，实际上如果搜索到最大数字为 9 就不需要继续搜索了，这可以进行一定的剪枝，但是我的实现用了 for_each 就具体实现了。

看到一个基本思路一致，但是实现上不同的方法 [ropewalker](https://www.reddit.com/r/adventofcode/comments/1pcvaj4/comment/ns0smcw) [code](https://github.com/ropewalker/advent_of_code_2025/blob/master/src/day03.rs) ，通过滑动限定长度窗口，依次取得最大值，实现不同但算法是一样的，效率应该也没有差别。

目前我的实现在查找过程中实际上存在大量的浪费，因为必须要比较到最后才能确定最大值，而每次查找最大值时都进行了一次。社区上也有人使用 DP 进行优化这个过程，即在一次比较中不断记录当前位置到末尾处的最大值，具体我就不实现了，做一下分析。

阅读社区上他人的动态规划代码 [michel-kraemer](https://github.com/michel-kraemer/adventofcode-rust/blob/main/2025/day03/src/main.rs)，这个实现使用的是自底向上的动态规划，过程中数组 dp 记录的是，长度为 len 的子串从 i 开始的最大子串，转移方程是 dp[len][i] = max(number[i]*mul + dp[len - 1][i + 1])，dp 从长度 1 以及从数字序列最后开始计算。

参考核心代码

```rust
fn dp_find_largest_joltage(battery: &[usize], number: usize) -> usize {
    let length = battery.len();
    let mut dp = vec![vec![0; length + 1]; number + 1];
    let mut mul = 1;
    for len in 1..=number {
        let mut max = 0;
        for (i, &b) in battery.iter().enumerate().take(length - len + 1).rev() {
            max = max.max(b * mul + dp[len - 1][i + 1]);
            dp[len][i] = max;
        }
        mul *= 10;
    }
    dp[number][0]
}
```

理论上 dp 应该要能减少比较的次数，但是这个实现实际的运行效率远不如我的暴力法，对比核心代码实际可以发现比较的次数并没有减少，虽然运行时都是 O(Nk)，N是序列长度，k是子串长度，但是增加了数组操作，整体反而增加了计算成本。比较次数没有减少，是因为每次计算 dp 时，依旧需要从初始位置开始比对到当前的结尾位置，冗余的比较依旧存在。

动态规划算法的核心

- dp[len][i]=从 battery[i..] 中选择 len 个数字，保持相对顺序串联得到的最大值。
- 边界条件（已初始化）：dp[0][i]=0：从任何位置开始选择 0 个数字，结果都是 0。
- 外部循环：按长度 (len) 迭代 `for len in 1..=number` 这个循环是按构建数字的位数从小到大进行计算。DP 算法的关键是利用小规模子问题的解来推导大规模问题的解。在这里，要计算 dp[len][i]，我们必须先知道 dp[len−1][j] 的值。
- 内部循环：按起始索引 (i) 迭代 `for (i, &b) in battery.iter().enumerate().take(length - len + 1).rev()`
- 优化原理：当我们在计算 dp[len][i] 时，我们在考虑从 battery[i..] 中选择 len 个数字。有两种情况：
    - 选择 battery[i] 作为第一个数字： 得到的数字是 battery[i]⋅mul+dp[len−1][i+1]。
    - 不选择 battery[i] 作为第一个数字： 这意味着第一个数字是从 battery[i+1..] 中选的，所以这等价于 dp[len][i+1]。

当前解决的这个特定问题，贪心算法在运行效率（常数时间和空间）上是更优的选择。但在算法设计的普适性上，DP 算法则更为强大。

## Day 04

今天的问题简洁明了，输入是二维的矩阵，其中每个位置上可能存在一卷纸，如果一个卷纸的八个邻接位置上只有小于三个位置上是卷纸时，叉车可以将卷纸移除。第一部分是移除一次，第二部分则是反复进行操作，直到矩阵中所有的卷纸都无法再被移除。问题的核心就是计算矩阵中每个卷纸邻接卷纸的个数，然后根据需求修改矩阵状态。

最直接的思路就是利用二位数组，直接将输入的字符串，转为二维字符数组，接着遍历每一个位置，计算八个邻接位置，判断其中是卷纸的数量。第二部分则需要多次遍历，每次遍历中需要记录移除的卷纸坐标，当一次遍历完成后，根据记录的坐标修改矩阵，将卷纸移除。思路简单明了，也得到了正确的答案，但是我对运行效率不满意，第一部分运行时间是 22ms ，而第二部分则是 273 ms。

第一部分的时间复杂度是 O(N) 其中 N 是矩阵大小，而第二部分的时间复杂度是 O(kN) 其中 k 时可能需要的遍历移除次数，k 最大为最初的卷纸数量。

二维数组模拟输入，实际上非常直观，但是数组存在使用上的局限性，比如八个邻接位置的判断，需要考虑边界条件。同时在这个问题中，第二部分需要搜索的矩阵大小实际上应该是逐渐减小的，而用数组表示则无法利用这个优势。用 HashSet 表示在实现上更加方便，集合只记录所有卷纸的坐标，搜索邻接八个位置时，只需要判断位置是否位于集合中，不必额外考虑坐标边界情况。虽然在实现上更加简便，但是效率却变差了，虽然时间复杂度没有变化，但是 HashSet 的操作更加耗时，最后两部分的运行时间分别是 45ms 和 1s。即便是优化了部分 HashSet 方法的使用，最后的运行时间依旧比不上数组，两部分分别只能做到 48ms 和 437 ms。从第一部分的运行时间差距可以发现，实际上 HashSet 的效率是远比不上数组访问。

### 初步优化（第二部分）

第一部分实际上没有什么优化空间，在这个问题中 Vec 的效率远比 Hash 效率高，优化的主要集中在第二部分。第二部分中移除是分批实现的，即先判断所有的卷纸是否能被移除，然后一次性移除，接着再次判断和移除。这样的实现可以确保结果正确性，但是能否判断一个就移除一个呢？因为所有的卷纸只可能会被移除，不会有新的卷纸产生，如果一个卷纸目前有四个邻接的卷纸，而其中一个卷纸被移除了，那么不需要等到下一次判断，这次就可以一并移除，虽然也会有漏网之鱼，但也能较大的提高运行效率。

实现这个优化采用二维矩阵比 HashSet 更加方便，因为在 Rust 中一边遍历 HashSet 一边又要删除元素是无法实现的（safe），一般来说也不推荐这样做，因为修改遍历对象属于遍历副作用，可能会导致循环无法结束。

在利用 Vec 实现这个优化之后，运行时间并没有大幅下降，第二部分运行时间是 173 ms，差不多少了 100ms。

### 进一步优化

运行效率不高的原因是，在每一个卷纸位置上进行搜索时，需要对领接的八个位置都进行判断，而这个过程又需要反复进行。如果能将搜索邻接位置的结果进行缓存，或者不需要每次都进行判断应该就能大幅降低运行时间。

考虑使用 HashMap 表示输入，其键为卷纸坐标，值为邻接的八个位置中有几个是卷纸。需要提前从输入构筑，初始的邻接搜索必须要进行。每一次搜索时，遇到一个卷纸位置，如果当前卷纸可以被移除，那么邻接八个位置在 HashMap 中的值就需要减 1 。

实际上也可以使用 Vec 二维数组实现，考虑 HashMap 可能存在操作的开销。但是这个思路可以进一步延伸，在搜索一个位置时，如果能提前判断邻接的八个位置是否能被移除，那就可以提前的确定这个卷纸是否能被移除。当然要规避如果 A 移除了，邻接的 B 就能被移除，同时如果 B 被移除，A 也就能被移除，这样的互相限制。

这样的优化终归是局部性的，并不是一劳永逸的，同时是有些盲目的操作。进一步我又想到，实现这个优化最好的数据结构是优先队列，键依旧是坐标，值依旧是邻接的卷纸数量。这个思路看似不错，但是并不容易，因为我需要快速的查找最小值，同时又要求 O(1) 的根据键并修改优先级。可以使用 HashMap + BinaryHeap 实现，HashMap 实时记录位置处邻接卷纸的数量，BinaryHeap 是最小堆，如果堆顶的值大于 4 则说明所有能被移除的卷纸已经被全部移除了。代码具体实现可见 [code](https://github.com/livexia/advent-of-code-2025/commit/3ada45bc2ee0c1cb4a47232aae5d158a766f7660#diff-9babad5b20f1c504f9c4b1d031fd57e6f1a1198343b707f377d777b64c50f47b)，第二部分的运行时间下降到 70ms。

### 使用 BFS 实现

前面的优化实际上就是更加复杂的 BFS ，但是我是在阅读了社区的题解之后才意识到可以使用 BFS 实现。

BFS 也需要用到两个数据结构，VecDeque 和 HashMap ，其中队列中保存的是当前矩阵中邻接卷纸数量小于 4 的卷纸坐标，而 HashMap 则不断记录着每个卷纸的邻接卷纸数量。

每次从队列中弹出卷纸时，更新这个卷纸的邻接卷纸的邻接数量，如果数量小于 4 则将该邻接卷纸入队，依次进行直到队列为空。具体实现见代码 [code](https://github.com/livexia/advent-of-code-2025/blob/main/aoc04/src/main.rs)，第二部分运行时间差不多是 70ms ，这个 BFS 实际上和前面的优化是一致的，所以在时间上没有快很多，但是算法更加简洁实现也更加容易。

因为暴力的方法很简单，所以没有单列与代码中，因为很久没写代码了，所以对于 BFS 有所模糊，最终的代码之保留了 BFS 的实现，BFS 中使用到的 Hash 数据结构也可以通过 Vec 替代，因为输入较小这样也可以提高实际的运行效率，同时提及的运行时间都是 debug 下得到的，release 的运行时间会更少。

## Day 5

今天是区间题，输入分为两个部分，第一部分为区间列表，第二部分为现有 id 列表。第一部分的题目要求计算有多少 id 落在给定的区间中。第一部分简单的暴力循环遍历即可，同样也可以将区间列表合并然后排序，再根据区间起点对 id 进行二分搜索，以降低运行时间提高运行效率，见 [代码](https://github.com/livexia/advent-of-code-2025/commit/f3f498a089d29dda64e26aba6e07ada9a0ccf158)。

第二部分则需要计算所有区间列表中可能的所有 id ，当然不允许重复的 id 。因为给定的区间存在大量重叠的情况，而且由于实际的输入区间较多且较大，那么通过遍历所有区间可能的 id 就不现实了。那么就需要合并区间，而要合并所有区间的第一步就是合并两个区间。

### 合并两个区间

首先根据区间的起点排序并交换给定的两个区间 A 和 B，区间 A 为起点较小的区间，那么如果 A 的终点大于或等于区间 B 的终点，那么合并后的区间就是 A ，如果 A 的终点小于 B 的终点，那么合并后的区间就是 A 的起点到 B 的终点。

题目的**区间是两头都封闭的**，那么如果两个区间头尾相接，则这两个区间也可以合并。
代码

```rust
fn merge_range(r: IdRange, other: IdRange) -> Option<IdRange> {
    let (r, other) = if r.0 > other.0 {
        (other, r)
    } else {
        (r, other)
    };
    if r.1 + 1 < other.0 {
        None
    } else {
        Some((r.0, r.1.max(other.1)))
    }
}
```

### 合并所有的区间

~~我一开始不想使用笨方法，但是简单的几次尝试之后发现都不行，于是使用笨方法得出了答案。存在两个队列，队列 A 是等待合并的区间，队列 B 是已经合并的区间。每次从队 A 弹出一个区间 a，循环遍历队列 B 的区间 b，直到 a 能和 b 合并，将合并后的区间放入队列 B ，直到队列 A 为空。然后交换队列 A 和 B 再次循环，直到没有任何新的合并发生，此时所有的区间都完成了合并，第二部分的答案只需要计算区间的长度即可。~~

经过一番搜索之后，我发现了一个合并所有区间的[算法](https://stackoverflow.com/a/5276789)，我才意识到太久没写代码让我的思维有些慢了，因为明明是很简单算法，没道理我不知道的。

算法

1. 排序 (Sort): 首先按区间的起始点进行排序。
2. 合并 (Merge): 然后遍历排序后的列表。
3. 循环：
    1. 初始区间 current 为排序后的第一个区间
    2. 从排序后的第二个区间开始遍历区间 next。
    3. 如果 current 和 next 可以合并为区间 m，令 current = m 循环继续。
    4. 如果不能合并，则将 current 放入已经完成合并的区间数组，同时令 current = next 循环继续。
        - 具体说明：如果不能合并，则说明 current 和 next 不存在重叠，由于区间是有序的，那么包括 next 之后的所有区间都无法和 current 合并。即 current 已经是确定的完成合并的区间，可以不必再考虑是否能和其他区间合并。
    5. 循环结束后，需要将最后的 current 放入已经完成合并的区间数组，避免遗漏。

代码

```rust
fn merge_ranges(ranges: &[IdRange]) -> Vec<IdRange> {
    let mut ranges = ranges.to_vec();
    ranges.sort();
    let mut merged = vec![];

    let mut current = ranges[0];

    for &next in &ranges[1..] {
        if let Some(m) = merge_range(current, next) {
            current = m;
        } else {
            merged.push(current);
            current = next;
        }
    }
    merged.push(current);
    merged
}
```

## Day 6

今天的题目有点陷阱题的意思，陷阱主要在于输入的处理上，输入类似于表格的形式，看到这个输入立马按照行进行了处理，将一个个数字和运算符解析出来，最终形成两个数组，这样处理之后轻松的就能完成第一部分。


### 列解析与逆序计算

第二部分的核心挑战在于必须采用**逆序的、基于列**的解析方式，而非传统的行处理。为了高效实现这一目标，我们对输入数据进行了预处理，并将运算逻辑与数据累积分离。

#### 1. 数据预处理（逆序化）

* **目标:** 实现从右到左的列迭代。
* **方法:** 对输入文本的每一行（去除空行后），将其**字节序列逆序**。
    * 例如：原输入行 `4 123` 预处理后变为 `[3, 2, 1, ' ' , 4]` (字节表示)。
* **结果:** `lines: Vec<Vec<u8>>` 数组中的列索引 $i$ （从 0 开始）现在对应原始输入中的**从右往左**的列。

#### 2. 核心计算流程（逐列迭代）

代码从 $i=0$ 开始，逐列向左迭代，并在每个列上执行两个主要动作：**数字提取**和**操作符判断**。

##### A. 数字提取 (`real`)

对于当前列 $i$:

1.  **范围:** 仅查看**操作符行之上**的所有行（即除了最后一行）。
2.  **过滤:** 忽略当前列中的所有**空格** (`b' '`) 字符。
3.  **组合:** 将当前列中，从上到下遇到的所有数字字符**组合成一个多位数字** `real`。
    * **公式:** 采用 **`fold(0, |r, n| r * 10 + n)`** 实现，其中 $r$ 是累积结果，$n$ 是当前行数字。

##### B. 操作符判断与计算

检查**最后一行**（操作符行）在当前列 $i$ 上的字符，根据其类型执行相应的逻辑。

| 最后一行字符 | 提取的 `real` 值 | 动作描述 | 对 `ans` 的影响 | `reals` 数组变化 |
| :--- | :--- | :--- | :--- | :--- |
| `b'+'` | 任何值 | **加法运算**。 | $ans += \text{sum}(\text{reals}) + \text{real}$ | 清空 (`reals.clear()`) |
| `b'*'` | 任何值 | **乘法运算**。 | $ans += \text{product}(\text{reals}) \times \text{real}$ | 清空 (`reals.clear()`) |
| 任何其他字符 | $> 0$ | **数字累积**。 | 无 | 推入当前数字 (`reals.push(real)`) |
| 任何字符 | $= 0$ | **序列中断**。 | 无 | 清空 (`reals.clear()`) |

> **`reals` 数组**：用于临时存储在一系列非操作符列中提取到的数字，直到遇到操作符为止。

#### 3. 结果

通过这种逐列、逆序的解析和即时计算，有效地解决了第二部分要求的计算逻辑，避免了复杂的中间数据结构转换，实现了高效的算法。

第二部分代码

```rust
fn part2<T: AsRef<str>>(input: T) -> Result<usize> {
    let _start = Instant::now();

    let mut ans = 0;
    let lines: Vec<Vec<_>> = input
        .as_ref()
        .lines()
        .filter(|l| !l.trim().is_empty())
        .map(|l| l.bytes().rev().collect())
        .collect();
    let op_row = lines.len() - 1;
    let mut reals = vec![];

    for i in 0..lines[0].len() {
        let real = lines[0..op_row]
            .iter()
            .filter_map(|row| {
                if row[i] == b' ' {
                    None
                } else {
                    Some((row[i] - b'0') as usize)
                }
            })
            .fold(0, |r, n| r * 10 + n);
        match lines[op_row][i] {
            b'+' => ans += reals.iter().sum::<usize>() + real,
            b'*' => ans += reals.iter().product::<usize>() * real,
            _ => {
                if real == 0 {
                    reals.clear();
                } else {
                    reals.push(real);
                }
            }
        }
    }

    println!("part 2: {ans}");
    println!("> Time elapsed is: {:?}", _start.elapsed());
    Ok(ans)
}
```
## Day 7

今天的题目难度适中，核心在于高效地追踪光束（beams）的分裂与传播。解题策略经历了从通用图搜索到**高效的行级遍历**的演变。

### 1\. 初始解法：图搜索策略 (BFS / DFS)

初始阶段，问题被视为图搜索问题，目的是覆盖所有可能路径并计算其组合：

#### 1\. BFS 部分：路径查找与去重

对于第一部分，关键在于追踪光束（beams）的移动路径并避免重复计算。

  * 我们使用 **BFS** 来确定所有可能被光束到达的位置。
  * 在 BFS 过程中，必须记录光束**已经到过的位置**，以防止**重复分割**或陷入**死循环**。这相当于使用一个集合（Set/Visited Array）来避免对同一状态的重复探索。

#### 2\. DFS 部分：结果缓存与优化

对于第二部分，目标是计算从每个分割点能产生的**平行世界总数**。

  * 我们使用 **DFS** 来递归地计算从任意分割处开始的路径总数。
  * 为提高效率，必须引入**中间结果缓存**（**Memoization** / 动态规划）。我们记录从**任意一个分割处**最终可能分割出的平行世界总数，从而避免重复计算相同子问题。

### 2\. 数据结构的选择与优化

在数据结构的选择上，我们进行了以下权衡：

  * 最初采用 **HashMap** 来表示输入数据是为了简化边界值判断。
  * 通过分析题目约束（光束**不会触发左右边界**，且**只会向下移动**），我们发现输入实际上是一个**稀疏矩阵**且移动方向受限。
  * 因此，理论上使用**二维向量 (`Vec<Vec<T>>`)** 的效率将**显著高于** HashMap，能带来更好的**空间局部性**和更快的访问速度。
  * **最终结论：** 尽管进行了数据结构替换，实际运行效率提升不明显，说明性能瓶颈主要在于**实际的运算逻辑**而非数据结构本身。

### 3\. 🚀 最终优化：行级遍历与输入特性利用 (Part 1)

通过分析问题特性，我们发现第一部分无需使用复杂的 BFS 进行状态追踪。由于光束只向下传播，我们可以采用**按行顺序遍历**的方法，**维护当前行光束的状态**，从而实现更高效的线性时间复杂度。

#### 核心算法逻辑：

我们维护一个布尔型数组 `beams`，其中 `beams[j] = true` 表示在当前行，有一束光会落到第 $j$ 列。

1.  **初始化：** `beams` 数组根据第一行的起始光束进行初始化。
2.  **按行迭代：** 遍历输入网格的每一行。
3.  **处理分割：** 对于当前行中的每个分割器（例如 `'^'`），如果该列存在光束 (`beams[j] == true`)，则**销毁原光束**并**生成左右两侧的新光束**，同时增加分割计数。

#### 优化点：利用输入的周期性

通过观察输入数据和题目要求，我们发现一个关键的优化机会：

  * **周期性：** 所有的分割器 (`splitter`) 都以**隔行**的方式出现。
  * **逻辑推论：** 一束光束遇到分割器并分裂后，下一行必定是**空行**（即不存在分割器）。这意味着光束遇到分裂器后，**至少需要跳过一行**才能再次遇到下一个可能的分裂器。
  * **效果：** 这种周期性允许我们使用 `step_by(2)` 来跳过空行，从而将实际迭代次数减少近一半。

利用这一特性，程序运行时间从约 $1 \text{ms}$ **大幅降低**至约 $80 \mu \text{s}$，体现了算法优化带来的巨大性能提升。

#### 核心代码展示 (Rust):

```rust
fn part1(grid: &Grid) -> Result<usize> {
    let _start = Instant::now();

    let mut count = 0;
    // 初始化第一行光束状态
    let mut beams: Vec<_> = grid[0].iter().map(|c| c == &'S').collect();

    // 利用 step_by(2) 跳过不包含 splitter 的空行，进一步优化性能
    for row in grid.iter().step_by(2) {
        // 迭代当前行的每一列
        for j in 0..beams.len() {
            // 如果存在光束且当前位置是分裂器
            if beams[j] && row[j] == '^' {
                beams[j] = false;       // 销毁原光束
                beams[j - 1] = true;    // 生成左侧新光束（利用题目约束：不触发边界）
                beams[j + 1] = true;    // 生成右侧新光束（利用题目约束：不触发边界）
                count += 1;
            }
        }
    }

    println!("part 1: {count}");
    println!("> Time elapsed is: {:?}", _start.elapsed());
    Ok(count)
}
```

### 4. 第二部分：动态规划与路径计数优化

第二部分的目标是计算从起始点到终点，所有可能产生的**平行世界（时间线）的总数**。虽然问题本质上是路径计数，但最优解法是采用\*\*自顶向下的动态规划（DP）\*\*思想，利用行级遍历实现高效计算。

#### 1\. 核心思想：动态规划路径累加

我们不再使用递归 DFS 加缓存 (Memoization)，而是采用**迭代式 DP**，按行顺序计算：

  * **状态定义：** 我们维护一个数组 `timelines`，其中 `timelines[j]` 记录了光束到达当前处理行时，**第 $j$ 列累计了多少条不同的时间线（路径）**。
  * **转移方程（分叉）：** 当光束在第 $j$ 列遇到分叉器 (`^`) 时，该列的路径数 `timelines[j]` 会被全部转移到下一行对应的左右两列。

#### 2\. 算法实现与状态维护

我们使用两个数组在每一步迭代中进行状态转移：

  * **`timelines` (Prev):** 记录**当前行**每个位置的累计时间线数。
  * **`next` (Current):** 记录这些时间线转移后，在**下一行**每个位置累计的时间线数。

在遍历当前行时：

1.  如果第 $j$ 列存在时间线且遇到分叉器 (`^`)，则将 `timelines[j]` 的值完整地累加到 `next[j - 1]` 和 `next[j + 1]`。
2.  如果第 $j$ 列存在时间线但**未**遇到分叉器，则将其值直接累加到 `next[j]`（即时间线笔直向下）。
3.  遍历完成后，用 `next` 数组更新 `timelines`，进入下一行计算。

#### 3\. 代码实现与性能

此迭代方法结合了输入数据的**隔行周期性**（利用 `grid.iter().step_by(2)` 优化），实现了 $O(N \cdot M)$ 的线性时间复杂度，并且避免了递归的开销。

#### 核心代码展示 (Rust):

```rust
fn part2(grid: &Grid) -> Result<usize> {
    let _start = Instant::now();

    // 初始状态：第一行只有'S'处有一条时间线，其余为0。
    let mut timelines: Vec<_> = grid[0].iter().map(|c| (c == &'S') as usize).collect();

    // 按行遍历，利用 step_by(2) 跳过不含 splitter 的空行。
    for row in grid.iter().step_by(2) {
        // next 数组存储下一行的状态，初始化为0
        let mut next = vec![0; timelines.len()];

        // 遍历当前行的所有时间线
        for (j, current_count) in timelines.iter().enumerate().filter(|(_, c)| c > &&0) {
            // current_count 是当前 j 列累计的时间线总数
            if row[j] == '^' {
                // 遇到分叉器，将路径数分配到左右两列
                next[j - 1] += current_count;
                next[j + 1] += current_count;
            } else {
                // 没有分叉器，路径数直行向下
                next[j] += current_count;
            }
        }
        // 更新状态，进行下一轮迭代
        timelines = next;
    }
    // 最终结果是最后一层所有列的时间线总数之和
    let count = timelines.iter().sum();

    println!("part 2: {count}");
    println!("> Time elapsed is: {:?}", _start.elapsed());
    Ok(count)
}
```

### 5. 总结与展望

尽管今天的程序在理论上仍有极致优化的空间——例如在**第一部分**引入**位掩码 (Bitmask)** 以利用 CPU 并行计算，或者在**第二部分**利用 **HashMap** (或稀疏索引) 来进一步压缩稀疏的时间线数据——但整体而言，目前的实现已经在代码可读性与运行效率之间取得了极佳的平衡。

值得注意的是，即便是最初构想的 BFS 和 DFS 方案，在当前的数据规模下也能保持不错的响应速度，并未出现严重的性能瓶颈。总体来看，今天的题目难度适宜，更像是一场帮助大家找回状态的**算法热身**。

## Day 8

题目输入包含一系列三维坐标，要求将这些点两两连接，且连接顺序必须按照坐标间的\*\*欧几里得距离（直线距离）\*\*由小到大进行。经过初步分析，这是一个典型的图论问题，非常适合使用 **并查集 (Union-Find / Disjoint Set Union)** 数据结构来解决。

由于需要重温算法细节，我查阅了 [维基百科](https://zh.wikipedia.org/wiki/%E5%B9%B6%E6%9F%A5%E9%9B%86) 并参考实现了并查集。

### 解题思路

首先，我们需要对数据进行预处理：生成所有可能的节点对（边），并计算它们之间的直线距离。随后，将这些边按照距离进行**升序排序**。

在并查集的实现中，我们使用一个数组 `circuits` 来维护节点的父子关系，其中 `circuits[i]` 表示节点 `i` 的父节点。

#### 初步尝试与遇到的问题

  * **第一部分**：要求在连接了距离最近的 $N$ 对节点后，计算所有连通分量的大小，并求出最大的三个连通分量大小的乘积。
  * **第二部分**：要求持续连接节点对，直到**所有节点都属于同一个连通分量**。此时，计算刚刚连接的那两个节点的 $x$ 坐标乘积。

最初，我采用了一种较为基础的并查集实现。为了计算连通分量的大小，我在每次合并操作后（或特定时刻），都需要遍历所有节点，通过 `find` 操作找到根节点，再利用 `HashMap` 统计每个根节点下的子节点数量。

这种方法在第一部分尚可接受，但在第二部分中显得效率极低。因为第二部分需要频繁检测“是否所有节点连通”，每次连接后都进行全量遍历和统计是不可接受的。

#### 算法优化：维护连通分量大小

为了解决性能瓶颈，我参考维基百科的思路，对并查集进行了微小的但关键的修改：**在并查集内部直接维护连通分量的大小**。

除了 `circuits` 数组外，我们新增一个 `sizes` 数组。

  * `sizes[i]` 表示以节点 `i` 为根的连通分量的大小。
  * **注意**：只有当 `i` 是根节点（即 `circuits[i] == i`）时，`sizes[i]` 的值才是准确且有意义的。

**查询 (Find) 操作：**
查询操作主要用于路径压缩，不涉及分量大小的变化。具体的实现中，我使用了标准的递归路径压缩，让节点直接指向根节点，从而加速后续查询。

```rust
fn find(&mut self, i: usize) -> usize {
    if self.parent[i] != i {
        // 路径压缩：递归找到根节点并更新当前父节点
        self.parent[i] = self.find(self.parent[i]);
    }
    self.parent[i]
}
```

**合并 (Union) 操作：**

这是优化的核心。当两个不同的连通分量合并时，采用了 “按大小合并” (Union by Size) 的策略：比较两个集合的大小，始终将较小的集合合并到较大的集合中。这不仅能保持树的平衡，还能防止树的高度过高。

```rust
fn union(&mut self, i: usize, j: usize) -> bool {
    let root_i = self.find(i);
    let root_j = self.find(j);

    // 只有当两个节点不在同一个集合时才合并
    if root_i != root_j {
        // 按大小合并：小树挂大树
        if self.size[root_i] < self.size[root_j] {
            self.parent[root_i] = root_j;
            self.size[root_j] += self.size[root_i];
        } else {
            self.parent[root_j] = root_i;
            self.size[root_i] += self.size[root_j];
        }
        return true;
    }
    false
}
```

### 最终方案

引入 `sizes` 数组后，两部分的求解变得非常高效：

1.  **第一部分**：连接指定数量的边后，只需遍历所有节点 `i`。如果 `circuits[i] == i`（即 `i` 是根节点），则 `sizes[i]` 即为该连通分量的大小。收集这些大小并排序即可求解。
2.  **第二部分**：在每次执行 `union` 操作并更新 `sizes` 后，直接判断当前新根节点的大小 `sizes[j_root]` 是否等于总节点数。如果相等，说明所有节点已连通，直接输出结果并终止循环。

### 总结

1.  **性能瓶颈分析**：本题的实际性能瓶颈在于**生成节点对并按距离排序**。这需要 $O(E \log E)$ 的时间复杂度（其中 $E$ 是边的数量，对于 $N$ 个点，$E \approx N^2$）。相比之下，优化后的并查集操作几近线性时间 $O(E \alpha(N))$，效率非常高。Rust 中使用 sort_unstable 进行排序比 sort 快，实际也是如此。

2.  **并查集原理回顾**：并查集之所以能保证“查找到同一个根”，归纳为以下三点核心机制：
    1.  **树形结构**：每个集合本质上是一棵树，所有节点的边都指向上级（父节点）。
    2.  **唯一出口**：每棵树只有一个根节点，且根节点的特征是指向自己（自环）。
    3.  **传递性**：所有的合并操作都是通过连接两个集合的**根节点**完成的。这保证了无论树的结构如何变化，子节点顺着父节点指针向上，最终一定能连通到新的根节点。

3. 今天的解题过程更像是一场“按图索骥”。虽然我第一时间就明确了应当使用并查集算法，但由于久疏战阵，对具体实现的细节记忆模糊，最终还是辅助了维基百科才完成代码。严格来说，这并不能算作完全的自主解题。题目本身难度适中，且并查集也是我过往多次学习和实践过的经典算法。这次的卡顿直观地反映出我编码熟练度的下滑，警示我未来需要加强基础算法的手写训练，避免眼高手低。

## Day 9

### 1\. 输入处理与类型迭代

在最初处理输入数据时，数据看起来只是普通的坐标点。但在着手解决第二部分的几何计算时，我意识到基础的整型可能无法满足需求。

Part 2 涉及大量的\*\*叉积（Cross Product）\*\*运算，其计算公式 $(B_x - A_x)(P_y - A_y) - ...$ 极易在中间步骤产生数值膨胀。为了确保后续复杂的几何运算绝对安全，避免任何潜在的整数溢出，我回过头将坐标点的类型定义调整为了 `(i128, i128)`。

这种基于核心算法需求反推数据结构选型的策略，为后续构建鲁棒的几何判断奠定了基础。输入解析部分则利用了 Rust 简洁的迭代器链（`filter`, `map`, `split_once`）完成，专注于将原始文本高效转换为强类型的坐标列表。

### 2\. Part 1: 无约束的最大矩形

第一部分的任务是在点集中找到能构成的最大矩形面积。这里的“面积”定义有些特殊，类似于计算像素网格的包围盒大小，即：
$$Area = (|x_1 - x_2| + 1) \times (|y_1 - y_2| + 1)$$

由于没有任何几何约束（不需要判断矩形是否被遮挡或在边界内），我直接采用了双重循环遍历所有点对。虽然时间复杂度是 $O(N^2)$，但在数据量允许的情况下，这是最直观且不易出错的解法。

### 3\. Part 2: 多边形内的最大矩形与几何算法

#### 解题思路：经典几何组合拳

当进入第二部分时，面对“判断矩形是否在多边形内”的问题，我选择了计算几何中最经典、最稳健的方案：**射线法定性 + 跨立实验定界**。

这个方案将问题拆解为两个维度：

1.  **点在哪里？** 确保矩形的顶点在多边形内部。
2.  **边怎么走？** 确保矩形的边没有穿过多边形的边界。

#### A. 核心算法实现

**1. 判定点在多边形内：射线法 (Ray Casting)**
为了确保矩形的四个顶点合法，我使用了射线法。

  * **原理**：从待测点向单一方向发射射线，统计与多边形边界的交点数量。奇数内，偶数外。
  * **实现细节**：利用 **叉积 (Cross Product)** 判断点与线段的相对位置，替代了不稳定的线性插值除法。
  * **参考资料**：
      * [Point in polygon - Wikipedia](https://en.wikipedia.org/wiki/Point_in_polygon)
      * [Linear interpolation - Wikipedia](https://en.wikipedia.org/wiki/Linear_interpolation)

**2. 判定边相交：跨立实验 (Straddle Test)**
为了确保矩形没有“穿墙”而出，我对矩形的四条边与多边形的所有边进行了相交检测。

  * **原理**：利用叉积判断两条线段是否相互跨越（即线段 A 的端点在线段 B 两侧，且线段 B 的端点在线段 A 两侧）。
  * **数学公式**：
    $$Cross(A, B, P) = (B_x - A_x)(P_y - A_y) - (B_y - A_y)(P_x - A_x)$$
  * **参考资料**：
      * [Line segment intersection - Wikipedia](https://en.wikipedia.org/wiki/Line_segment_intersection)
      * [Cross product - Wikipedia](https://en.wikipedia.org/wiki/Cross_product%23Computational_geometry)

#### B. 性能优化：AABB 坐标限制

考虑到题目输入数据具有 **正交性（Orthogonal，仅含水平/垂直线）**，我在跨立实验的基础上引入了 **AABB（轴对齐包围盒）** 逻辑作为一种性能优化。

对于正交线段，判断相交不需要计算昂贵的乘法叉积，只需比较坐标范围：

  * 如果 $Edge_x$ 在 $Rect_{x\_range}$ 之间，且 $Rect_y$ 在 $Edge_{y\_range}$ 之间，则视为相交（穿过）。

这种方法在本质上与跨立实验等价，但在正交场景下 CPU 指令更少，速度更快。

#### C. 算法局限性与边界陷阱 (Critical Limitations)

尽管使用了上述经典算法，我在复盘时发现这种判定逻辑在特定**凹多边形**场景下仍存在盲区。

**问题场景：完美凹陷填充 (The Perfect Dent)**
如果多边形有一个凹陷区域，而矩形恰好**完美填满**了这个凹陷（即矩形边与多边形凹陷边完全重合），现有的两种方法都会失效：

  * **射线法**：矩形四个顶点都在多边形边界上，判定为“合法”。
  * **跨立实验/AABB**：由于矩形边与多边形边**重合**而非**交叉**（不满足严格不等号），判定为“未相交”。
  * **结果**：算法误判矩形在内部，但实际上它在外部填补了空缺。

**潜在对策：中心点检测 (Center Point Check)**
一种可能的补救措施是检查矩形的**中心点**是否在多边形内。

  * 理论上，如果矩形填充了外部凹陷，其中心点必然在外部。
  * **决策**：虽然这是一个直觉上的补丁，但在复杂的拓扑结构中，几何完备性难以保证。这引导我关注社区中更系统化的“坐标离散化”方案。

### 4\. Rayon 并行计算优化

在解决几何算法问题后，我面临了严重的性能挑战。Part 2 需要对每一对点生成的矩形进行复杂的几何检测，计算量随着点数增加呈指数级上升。

作为一个 Rust 初学者，我不确定如何手动管理多线程。为了解决这个问题，我咨询了 AI 是否有简单的方法来加速循环。AI 推荐了 `rayon` 库。

虽然我对 `rayon` 内部具体的工作机制（如 Work-Stealing 线程池等）并不十分清楚，但它的使用体验非常惊艳：

  * **代码改动**：只需简单地将迭代器从 `.iter()` 改为 `.into_par_iter()`。
  * **效果**：程序自动利用了多核 CPU 资源，计算速度得到了数量级的提升。

这种“黑盒”式的优化体验让我印象深刻——在不深入了解并发编程复杂细节的情况下，依然能够通过成熟的社区库解决实际的工程瓶颈。配合内部循环中的算法剪枝（`if area <= local_largest`），最终成功在合理时间内跑出了结果。

### 5\. 社区精华 (Community Insights)

在完成题目后，我查阅了 Reddit 上的社区讨论，发现了一个**在正确性与效率上均优于我当前解法**的高阶方案体系：

  * **终极方案：坐标离散化 + 泛洪填充 + 单调栈**
    这套组合拳将几何问题转化为了纯粹的数据结构问题，彻底规避了复杂的浮点/边界判断。

      * **步骤 1：离散化 (Compression)**：将稀疏的坐标映射为紧凑的网格，消除无限空间，构建有限的矩阵。
      * **步骤 2：泛洪填充 (Flood Fill)**：从外部对网格进行染色。任何无法被染色的区域即为多边形内部。**这从拓扑上完美解决了“凹陷”、“空洞”等所有几何陷阱，保证了解的绝对正确性。**
      * **步骤 3：单调栈 (Monotonic Stack)**：将问题转化为经典的“直方图最大矩形 (Largest Rectangle in Histogram)”问题，利用单调栈在 $O(N)$ 时间内求出每一行的最优解。
      * **来源**：[Rust Solution by u/AxlLind](https://www.reddit.com/r/adventofcode/comments/1phywvn/comment/nt2oahn/)

  * **并行暴力解法的有效性**

      * **来源**：[Rust Solution by u/maneatingape](https://www.reddit.com/r/adventofcode/comments/1phywvn/comment/nt2ly9q/)

  * **常见的面积计算陷阱**

      * **来源**：[Python Analysis by u/wimg](https://www.reddit.com/r/adventofcode/comments/1phywvn/comment/nt2mk9f/)

### 6\. 总结与回顾 (Summary & Retrospective)

回顾今天的挑战，这不仅是一次编程练习，更是一场关于几何算法和工程优化的综合实战。

  * **核心算法决策 (Core Algorithms)**:
    本题我采用了 **射线法 + 跨立实验** 的正统几何解法。这是一种符合直觉且易于实现的方案，但在处理“完美凹陷”等极端拓扑结构时存在盲区。这让我意识到几何计算中“边界判定”与“区域判定”的本质区别。

  * **未来的学习方向 (Future Outlook)**:
    通过对比社区的高分回答，我发现 **“坐标离散化 + 泛洪填充 + 单调栈”** 才是此类问题的“版本答案”。

      * **正确性**：它通过泛洪填充建立了严格的拓扑内外关系，无视几何形状的复杂性（彻底消灭 L型陷阱、C型空洞）。
      * **效率**：它将 $O(N^3)$ 的暴力枚举优化为了接近线性或 $O(N^2)$ 的网格扫描，效率有质的飞跃。
        尽管这次我使用了 Rayon 暴力破解了 Part 2，但这套高阶算法逻辑严密且优雅，是我未来深入学习计算几何时的重点目标。

  * **Rust 工程化**:
    从 `i128` 的防溢出选择，到 `Rayon` 的并行加速，展示了 Rust 在处理计算密集型任务时强大的语言表现力和生态支持。
